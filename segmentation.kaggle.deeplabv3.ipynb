{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip install git+https://github.com/qubvel/segmentation_models.pytorch.git@master","metadata":{"execution":{"iopub.status.busy":"2021-09-27T19:33:52.059389Z","iopub.execute_input":"2021-09-27T19:33:52.060055Z","iopub.status.idle":"2021-09-27T19:34:06.416202Z","shell.execute_reply.started":"2021-09-27T19:33:52.059956Z","shell.execute_reply":"2021-09-27T19:34:06.415416Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Common\n\nCommon part is a prerequisite for all further parts (data loading, training, inference).","metadata":{"id":"e_ZPZcHtaK47"}},{"cell_type":"markdown","source":"## Add dataset and save model weights\nFile -> Add or upload data","metadata":{}},{"cell_type":"markdown","source":"## Paths\n\n`PATH_OUTPUT` is used to save images (for example, inference on a test subset).\n\n`PATH_MODELS` is used to save model weights for later inference.\n\n`PATH_RESUME` is used to copy model weights for later inference.\n\n`PATH_DATASET` is a source of GeoTIFF **images** and **masks** for training and inference (for example, test subset).","metadata":{"id":"nSdXi6F2aWMs"}},{"cell_type":"code","source":"from os import path as osp\nimport os \n\nNOTEBOOK_NAME = ''\n\nPATH_MODELS = osp.join('/', 'kaggle', 'working', 'models')\nPATH_RESUME = osp.join('/', 'kaggle', 'input', NOTEBOOK_NAME, 'models')\nPATH_DATASET = osp.join('/', 'kaggle', 'input', 'sentinel1', 'dataset')\nPATH_OUTPUT  = osp.join('/', 'kaggle', 'working', 'output')\n\nprint('\\n'.join((NOTEBOOK_NAME, PATH_MODELS, PATH_DATASET, PATH_RESUME, PATH_OUTPUT)))","metadata":{"id":"e4vFyT2LTzN7","execution":{"iopub.status.busy":"2021-09-27T19:34:08.556682Z","iopub.execute_input":"2021-09-27T19:34:08.556953Z","iopub.status.idle":"2021-09-27T19:34:08.565733Z","shell.execute_reply.started":"2021-09-27T19:34:08.556923Z","shell.execute_reply":"2021-09-27T19:34:08.565048Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from shutil import copytree\n\nif osp.exists(PATH_RESUME) and not osp.exists(PATH_MODELS):\n    copytree(PATH_RESUME, PATH_MODELS)\nos.makedirs(PATH_MODELS, exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T19:34:09.753289Z","iopub.execute_input":"2021-09-27T19:34:09.753900Z","iopub.status.idle":"2021-09-27T19:34:09.761558Z","shell.execute_reply.started":"2021-09-27T19:34:09.753863Z","shell.execute_reply":"2021-09-27T19:34:09.760767Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Functions\n\nAuxiliary functions (for example, drawing/plotting data).","metadata":{"id":"-vIWdMcLXMxZ"}},{"cell_type":"code","source":"import os\n\nfrom matplotlib import pyplot as plt\n\n\ndef draw_one_row(*images, size=1024, output=None):\n    try:\n        size = size[:2]\n    except:\n        size = (size, size)\n    count = len(images)\n    figure, axes = plt.subplots(1, count, dpi=72,\n                                figsize=(size[0] / 72, size[1] / 72))\n    for i in range(count):\n        axes[i].imshow(images[i])\n    if output is not None:\n        try:\n            os.makedirs(osp.dirname(output), exist_ok=True)\n            plt.savefig(output)\n        except:\n            pass\n    plt.show()","metadata":{"id":"S-CGHBQFXQ44","execution":{"iopub.status.busy":"2021-09-27T19:34:21.285779Z","iopub.execute_input":"2021-09-27T19:34:21.286327Z","iopub.status.idle":"2021-09-27T19:34:21.293504Z","shell.execute_reply.started":"2021-09-27T19:34:21.286290Z","shell.execute_reply":"2021-09-27T19:34:21.292849Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Config\n\nConfiguration dictionary to store training parameters. This config dictionary will be updated in subsequent cells.","metadata":{"id":"FhMnFHJsUrIb"}},{"cell_type":"code","source":"config = {\n    'classes': ['nodata', 'water', 'ice'],\n    'batch_size_train': 1,\n    'batch_size_valid': 1,\n    'num_workers_train': 1,\n    'num_workers_valid': 1,\n    'model_encoder': 'resnet34',\n    'model_pretrain': 'ImageNet',\n    'model_channels': 3,\n    'data_split': 1,\n    'expand': True,\n    'debug': False,\n    'flat': True,\n}","metadata":{"id":"jnRF2Gc_U7Gi","execution":{"iopub.status.busy":"2021-09-27T19:34:31.728236Z","iopub.execute_input":"2021-09-27T19:34:31.728520Z","iopub.status.idle":"2021-09-27T19:34:31.733335Z","shell.execute_reply.started":"2021-09-27T19:34:31.728492Z","shell.execute_reply":"2021-09-27T19:34:31.732686Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Data Loading\n","metadata":{"id":"y52v0cNRKAs6"}},{"cell_type":"markdown","source":"## Dataset class\n\nThe most interesting part of the pipeline, 'cause it's an interface between data and the neural network models. Here is an example of a dataset class (`DatasetSAR`) which loads a pair of HH+HV images and their corresponding masks from different directories, stacks them together, applies goemetrics transformations and color augmentations (here the general _augmentations_ term was split into geometric and color in such a way).\n\nParameters for the `DatasetSAR` class constructor:\n\n* `paths_images` is a list of strings (or a single string in case of only one polarization used) where each string point to an existing directory with HH or HV polarized images;\n* `paths_masks` is a list of strings / single string that point(s) to where masks are located (only first directory from the list is used);\n* `classes` is a list of classes used — important for multiclass masks (where they should be **expanded** into one-hot encoded tensor);\n* `items` is a list of filenames to be used as the dataset items (WARNING: passed list of filenames is acceted as-is with no checks) — useful for creating subsets (for example, train/valid) from some superset (list of all items of the dataset);\n* `transformations` is a [torchvision transforms](https://pytorch.org/vision/stable/transforms.html) class (or any other compatible callable class) **without** things like `ToTensor` (it is added separately to the images only) that is applied to both images and masks;\n* `augmentations` is a callable like `transformations`, but is applied to images only;\n* `size` is an integer or 2x tuple of integers for final image/mask output size (for example, 1024x1024 as input image for Unet with EfficientNet-B2 encoder);\n* `expand` is a boolean flag which true value enables expanding a mask into one-hot encoded tensor (that is required for some loss functions and their modes, default is `False` — do not expand);\n* `flat` is a boolean flag which true value produces squeezed mask (`(H, W)` instead of `(C, H, W)`, for example, for Focal Loss target) — takes precedence over `expand` flag (if `flat` then it's never `expand`ed).\n\n> DEBUG: commented line with `np.clip` is for testing binary segmentation.\n\n> TODO: add loading and geometric transformations for RGB images.","metadata":{"id":"iaDLWDQsmbyK"}},{"cell_type":"code","source":"from collections import OrderedDict\nfrom glob import glob\n\nimport cv2 as cv\nimport numpy as np\nimport torch\n\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nfrom torchvision.transforms import Compose, Lambda, RandomResizedCrop, Resize, ToTensor\n# from torchvision.transforms.functional import InterpolationMode\nfrom PIL import Image as InterpolationMode\n\n\n\nclass DatasetSAR(Dataset):\n    def __init__(self, paths_images, paths_masks, classes, items=None,\n                 transformations=None, augmentations=None, size=None,\n                 expand=False, flat=False):\n        # Assert images paths\n        if isinstance(paths_images, str):\n            paths_images = (paths_images, path_images)\n        elif not isinstance(paths_images, (tuple, list, set)):\n            raise TypeError(\"first argument must be of type str or list!\")\n        else:\n            paths_images = tuple(paths_images)\n        # Assert masks paths\n        if isinstance(paths_masks, str):\n            paths_masks = (paths_masks,)\n        elif not isinstance(paths_masks, (tuple, list, set)):\n            raise TypeError(\"second argument must be of type str or list!\")\n        else:\n            paths_masks = tuple(paths_masks)\n        # Default output image/mask size\n        if size is None:\n            size = (1024, 1024)\n        # Default transformations (geometric - image + mask)\n        mode_interpolation = InterpolationMode.NEAREST\n        if not isinstance(transformations, (Compose, torch.nn.Module)):\n            self.transformations = Compose([\n                Resize(size, mode_interpolation),\n            ])\n        else:\n            self.transformations = transformations\n        # Default augmentations (color - image only)\n        if not isinstance(augmentations, (Compose, torch.nn.Module)):\n            self.augmentations = Compose([\n                Lambda(lambda x: x),\n            ])\n        else:\n            self.augmentations = augmentations\n        # Class attributes\n        self.to_tensor = ToTensor()\n        self.paths_images = paths_images\n        self.paths_masks = paths_masks\n        if items is None:\n            items = []\n            for path in paths_images[:2] + paths_masks[:1]:\n                items.append({osp.basename(item) for item \\\n                            in glob(osp.join(path, '*.tiff'))})\n            self.items = sorted(items[0].intersection(*items))\n        else:\n            self.items = items\n        self.classes = len(classes)\n        self.items_class = OrderedDict({c: i for c, i \\\n                                        in zip(classes,\n                                               range(1, self.classes + 1))})\n        self.expand = expand\n        self.flat = flat\n        return None\n\n    def __getitem__(self, item):\n        # TODO: load RGBs\n        image_hh = cv.imread(osp.join(self.paths_images[0], self.items[item]),\n                             cv.IMREAD_LOAD_GDAL | cv.IMREAD_GRAYSCALE)\n        h, w = image_hh.shape[:2]\n        image_hv = cv.imread(osp.join(self.paths_images[1], self.items[item]),\n                             cv.IMREAD_LOAD_GDAL | cv.IMREAD_GRAYSCALE)\n        h, w = tuple(map(min, zip(image_hv.shape[:2], (h, w))))\n        if self.paths_masks:\n            mask = cv.imread(osp.join(self.paths_masks[0], self.items[item]),\n                            cv.IMREAD_LOAD_GDAL | cv.IMREAD_GRAYSCALE)\n        else:\n            mask = image_hh\n        h, w = tuple(map(min, zip(mask.shape[:2], (h, w))))\n        # Use minimum height and width 'cause image/mask dimension may not\n        # always fit (difference during mask conversion)\n        image = Image.fromarray(np.dstack(\n            [image_hh[:h, :w], image_hv[:h, :w], mask[:h, :w]]\n            # [image_hh[:h, :w], image_hv[:h, :w], np.clip(mask[:h, :w], 0, 1)]\n        ))\n        del image_hh, image_hv, mask\n        image = self.transformations(image)\n        image = np.array(image)\n        image, mask = image[..., :2], image[..., 2]\n        image = self.to_tensor(np.dstack((image[..., 1], image)))\n        image = self.augmentations(image)\n        # Convert to int64 'cause OHE requires index tensor\n        if self.flat:\n            mask = torch.tensor(mask)\n        elif self.expand:\n            mask = torch.nn.functional.one_hot(torch.tensor(mask,\n                                                            dtype=torch.int64),\n                                               self.classes).to(torch.int8)\\\n                                               .permute(2, 0, 1)\n        else:\n            mask = torch.tensor(mask).unsqueeze_(0).to(torch.int64)\n        return image, mask\n\n    def __len__(self):\n        return len(self.items)\n\n    def debug(self):\n        # Debug function to show some dataset stats\n        print(f\"DEBUG: paths images = {self.paths_images}\")\n        print(f\"DEBUG: paths masks = {self.paths_masks}\")\n        print(f\"DEBUG: items ({len(self.items)}):\")\n        print('\\n'.join(self.items))\n        print(f\"DEBUG: classes ({self.classes}):\")\n        print(self.items_class)","metadata":{"id":"EHVKXKfwKF2d","execution":{"iopub.status.busy":"2021-09-27T19:34:34.367401Z","iopub.execute_input":"2021-09-27T19:34:34.367753Z","iopub.status.idle":"2021-09-27T19:34:39.025776Z","shell.execute_reply.started":"2021-09-27T19:34:34.367722Z","shell.execute_reply":"2021-09-27T19:34:39.025045Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Datasets and visualization\n\nThis part produces datasets to be used in data loaders for training and validation. Here are three datasets: `dataset` — grand superset with all annotated images; `dataset_train` — training subset of grand dataset to be used during training (calculating gradients); `dataset_valid` — validation subset of grand dataset to be used during validation (calculating metrics).\n\n> Training/validation items (`items_train`/`items_valid`) are selected evenly across the grand dataset items (feel free to split it in any other way).\n\nTransformations are also in two variants (train/valid): **random crop and resize** for training in order to learn features from different scales and simple **resize** for validation (for simplicity and closer approach to later inference scenarios).\n\n> All transformations suppose output shape of images/masks to be 1024x1024 (set by `size` valiable).\n\nImages and masks from train/valid dataset classes are visualized in the last two loops. As for _MaritimeAI Sentinel-1 Dataset 1920_, there will be displayed 124 images and corresponding masks (neither too much nor too little).\n\nVariables `EXPAND` and `FLAT` are used for top-level control of mask output format. For example, for Dice loss from _Segmentation Models_ _utils_ module set `EXPAND=True` and `FLAT=False`, for Dice loss from _Segmentation Models_ _losses_ module set all two variables `False`, for Focal loss from _losses_ module set `FLAT=True`.\n\nVariable `DEBUG` sets debug mode for overfitting on one image.\n\nClass list used for two-class segmentation (from _MaritimeAI Sentinel-1 Dataset 1920_ `2-class` directory) includes: `nodata` (zero values), `water` (ones) and `ice` (twos).\n\n> Formally, there are only `water` and `ice` classes of interest, but technically, all three classes must be specified.\n\n> DEBUG: for true binary segmentation there should be one class (for example, `data`).\n\n> Yet another variable `NUM_SAMPLES` had been added to control the number of output samples from training and validation datasets (preview data loading transformations and augmentations).","metadata":{"id":"4fHWXrItk1CS"}},{"cell_type":"code","source":"items_exclude_a = [\n    'S1B_EW_GRDM_1SDH_20200203T031613_20200203T031631_020099_0260A6_D03B.tiff',\n    'S1B_EW_GRDM_1SDH_20200215T031613_20200215T031630_020274_026647_9E25.tiff',\n    'S1B_EW_GRDM_1SDH_20200227T031613_20200227T031630_020449_026BE9_3282.tiff',\n    'S1B_EW_GRDM_1SDH_20200310T031613_20200310T031630_020624_027178_1A36.tiff',\n    'S1B_EW_GRDM_1SDH_20200322T031613_20200322T031631_020799_027702_664C.tiff',\n    'S1B_EW_GRDM_1SDH_20200521T031615_20200521T031633_021674_029249_923C.tiff',\n]\n\nitems_exclude_b = [\n    'S1A_EW_GRDM_1SDH_20191117T031700_20191117T031800_029945_036ADD_32F2.tiff',\n    'S1A_EW_GRDM_1SDH_20191129T031659_20191129T031759_030120_0370EF_D07E.tiff',\n    'S1A_EW_GRDM_1SDH_20200104T031658_20200104T031758_030645_038306_DDA1.tiff',\n    'S1A_EW_GRDM_1SDH_20200328T031656_20200328T031756_031870_03ADB5_D992.tiff',\n    'S1A_EW_GRDM_1SDH_20200421T031657_20200421T031757_032220_03BA08_1F43.tiff',\n]\n\nitems_exclude_c = [\n    'S1A_EW_GRDM_1SDH_20191211T031659_20191211T031759_030295_0376F4_BE3E.tiff',\n    'S1A_EW_GRDM_1SDH_20191223T031658_20191223T031758_030470_037CFD_AB38.tiff',\n    'S1A_EW_GRDM_1SDH_20200221T031656_20200221T031756_031345_039B6D_927B.tiff',\n    'S1A_EW_GRDM_1SDH_20200304T031656_20200304T031756_031520_03A17D_08EB.tiff',\n    'S1A_EW_GRDM_1SDH_20200316T031656_20200316T031756_031695_03A78C_D3A3.tiff',\n    'S1A_EW_GRDM_1SDH_20200409T031657_20200409T031757_032045_03B3E6_7A01.tiff',\n    'S1A_EW_GRDM_1SDH_20200503T031658_20200503T031758_032395_03C031_950B.tiff',\n]\n\nitems_exclude_d = [\n    'S1A_EW_GRDM_1SDH_20191107T030034_20191107T030132_029799_0365BB_F7CF.tiff',\n    'S1B_EW_GRDM_1SDH_20200601T023525_20200601T023601_021834_02971A_B08C.tiff'\n]","metadata":{"id":"lUcO6jcM7jzf","execution":{"iopub.status.busy":"2021-09-27T19:34:39.027441Z","iopub.execute_input":"2021-09-27T19:34:39.027719Z","iopub.status.idle":"2021-09-27T19:34:39.034996Z","shell.execute_reply.started":"2021-09-27T19:34:39.027685Z","shell.execute_reply":"2021-09-27T19:34:39.033077Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from random import sample\n\n\nCLASSES = config['classes']\n# CLASSES = ['data']  # do np.clip(mask, 0, 1) for binary mode\nNUM_SAMPLES = 6  # draw samples from train/valid datasets\nDATA_SPLIT = config['data_split'] - 1 if 'data_split' in config else None\n\nDEBUG = config['debug']\nEXPAND = config['expand']\nFLAT = config['flat']\n\n# Nearest interpolation mode is mandatory for masks\n# and optional (but recommended) for images\nmode_interpolation = InterpolationMode.NEAREST\n\n# Separate transformations for train and valid\nsize = (1024, 1024)  # size for network input\ntransformations_train = Compose([\n    RandomResizedCrop(size, (0.25, 0.95), (3 / 4, 4 / 3),\n                      mode_interpolation),\n])\n\ntransformations_valid = Compose([\n    Resize(size, mode_interpolation),\n])\n\n# Images/masks paths\npath_images_hh = osp.join(PATH_DATASET, 'images', 'hh')\npath_images_hv = osp.join(PATH_DATASET, 'images', 'hv')\npath_masks = osp.join(PATH_DATASET, 'masks', '2-class')\n\n# Grand dataset\ndataset = DatasetSAR((path_images_hh, path_images_hv), path_masks,\n                     classes=CLASSES)\n\n# Split the dataset into train/valid datasets\nitems_total = len(dataset)\nfraction = 100 / 15\n\n# Train/valid items (images and masks)\nif DATA_SPLIT is None:\n    items_train = tuple(dataset.items[i] for i in range(items_total) \\\n                        if not i or i % int(round(fraction)))\n\n    items_valid = tuple(dataset.items[i] for i in range(items_total) \\\n                        if i and not i % int(round(fraction)))\nelse:\n    items_all = [item for item in sorted(dataset.items) if item not in \n                 items_exclude_a + items_exclude_b + items_exclude_c +\n                 items_exclude_d]\n    items_split = [{\n        'train': sorted(set(items_all) - set(items_all[i::5])),\n        'valid': sorted(items_all[i::5])\n    } for i in range(5)]\n    items_train = items_split[DATA_SPLIT]['train']\n    items_valid = items_split[DATA_SPLIT]['valid']\n\n# Debug: set default transformations for training and validation\nif DEBUG:\n    transformations_train = None\n    transformations_valid = None\n\ndataset_train = DatasetSAR((path_images_hh, path_images_hv), path_masks,\n                           items=items_train, classes=CLASSES, expand=EXPAND,\n                           flat=FLAT, transformations=transformations_train)\n\ndataset_valid = DatasetSAR((path_images_hh, path_images_hv), path_masks,\n                           items=items_valid, classes=CLASSES, expand=EXPAND,\n                           flat=FLAT, transformations=transformations_valid)\n\n# Debug: make datasets of one item\nif DEBUG:\n    dataset_train.items = dataset.items[:1]\n    dataset_valid.items = dataset.items[:1]\n\nnomasks = []\nsize_train = len(dataset_train)\nprint(f\"Training dataset ({size_train}):\")\nfor i in sample(range(size_train), k=min(NUM_SAMPLES, size_train)):\n    try:\n        image, mask = dataset_train[i]\n        draw_one_row(image.permute(1, 2, 0), mask if FLAT else mask.argmax(0) \\\n                     if EXPAND else mask.squeeze(0) if not FLAT else mask)\n    except AttributeError:\n        nomask = dataset_train.items[i]\n        nomasks.append(nomask)\n        print(f\"ERROR: failed to load {nomask}\")\nprint(f\"Read errors = {len(nomasks)}\")\n\nconfig['items_train'] = dataset_train.items\nconfig['errors_train'] = nomasks\nconfig['size_train'] = size_train\n\nnomasks = []\nsize_valid = len(dataset_valid)\nprint(f\"Validation dataset ({size_valid}):\")\nfor i in sample(range(size_valid), k=min(NUM_SAMPLES, size_valid)):\n    try:\n        image, mask = dataset_valid[i]\n        draw_one_row(image.permute(1, 2, 0), mask if FLAT else mask.argmax(0) \\\n                     if EXPAND else mask.squeeze(0))\n    except AttributeError:\n        nomask = dataset_valid.items[i]\n        nomasks.append(nomask)\n        print(f\"ERROR: failed to load {nomask}\")\nprint(f\"Read errors = {len(nomasks)}\")\n\nconfig['items_valid'] = dataset_valid.items\nconfig['errors_valid'] = nomasks\nconfig['size_valid'] = size_valid","metadata":{"id":"qXZSfhywqlGY","execution":{"iopub.status.busy":"2021-09-27T19:34:39.036393Z","iopub.execute_input":"2021-09-27T19:34:39.036680Z","iopub.status.idle":"2021-09-27T19:35:06.352111Z","shell.execute_reply.started":"2021-09-27T19:34:39.036647Z","shell.execute_reply":"2021-09-27T19:35:06.351414Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"`DatasetSAR` class has optional `debug` method to show filenames of items and some other info.","metadata":{"id":"nQvSiMDNy7zJ"}},{"cell_type":"code","source":"# dataset.debug()","metadata":{"id":"hWXaIVDeraup","execution":{"iopub.status.busy":"2021-09-27T19:35:06.354193Z","iopub.execute_input":"2021-09-27T19:35:06.354651Z","iopub.status.idle":"2021-09-27T19:35:06.358262Z","shell.execute_reply.started":"2021-09-27T19:35:06.354614Z","shell.execute_reply":"2021-09-27T19:35:06.357598Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Dataloaders\n\n_Google Colab_ settings. Batch size can be increased, also number of workers, if neccessary.","metadata":{"id":"75EX9_pIqPo8"}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\n\nloader_train = DataLoader(dataset_train, batch_size=config['batch_size_train'],\n                          shuffle=True, num_workers=config['num_workers_train'])\n\nloader_valid = DataLoader(dataset_valid, batch_size=config['batch_size_valid'],\n                          shuffle=False, num_workers=config['num_workers_valid'])","metadata":{"id":"eLyc1dHhmHZO","execution":{"iopub.status.busy":"2021-09-27T19:35:06.359658Z","iopub.execute_input":"2021-09-27T19:35:06.360178Z","iopub.status.idle":"2021-09-27T19:35:06.368490Z","shell.execute_reply.started":"2021-09-27T19:35:06.360142Z","shell.execute_reply":"2021-09-27T19:35:06.367843Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{"id":"_wTOlKWdKDc1"}},{"cell_type":"markdown","source":"## W&B\n\nAn API token is required in order to authorize in WandB:\nhttps://wandb.ai/authorize","metadata":{"id":"nux0P43rH6qN"}},{"cell_type":"code","source":"%pip install --upgrade wandb","metadata":{"id":"nWczlMT8H58m","execution":{"iopub.status.busy":"2021-09-27T19:35:06.369466Z","iopub.execute_input":"2021-09-27T19:35:06.369716Z","iopub.status.idle":"2021-09-27T19:35:15.786758Z","shell.execute_reply.started":"2021-09-27T19:35:06.369684Z","shell.execute_reply":"2021-09-27T19:35:15.785627Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Add-ons -> secrets -> new secrets -> label('wandb') -> value(https://wandb.ai/authorize)","metadata":{}},{"cell_type":"code","source":"import wandb\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"wandb\")\n\nwandb.login(key=secret_value_0)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T19:35:15.788700Z","iopub.execute_input":"2021-09-27T19:35:15.788999Z","iopub.status.idle":"2021-09-27T19:35:16.931208Z","shell.execute_reply.started":"2021-09-27T19:35:15.788959Z","shell.execute_reply":"2021-09-27T19:35:16.930488Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Model\n\n**Unet** with **EfficientNet-B2** encoder is used as an example 'cause it does fit into _Google Colab_ GPU RAM. EfficientNet-B3 may also fit (larger encoders like EfficientNet-B7 were tested on local devboxes and gave the best results).\n\nFor some losses from _Segmentation Models_ it may require to change mask output format (squeezed or one-hot expanded, see `EXPAND` and `FLAT` variables for datasets).\n\nExperimenting with **loss functions** may give interesting results. Compare the same scenario with different loss functions:\n\n* DiceLoss (with classes 1 and 2)\n\n![DiceLoss](https://raw.githubusercontent.com/MaritimeAI/ODS-SoC/1b066dd63aedd364fda4ad0d08ac3ae6286f2289/assets/images/dice_12.png)\n\n* FocalLoss\n\n![FocalLoss](https://raw.githubusercontent.com/MaritimeAI/ODS-SoC/1b066dd63aedd364fda4ad0d08ac3ae6286f2289/assets/images/focal_all.png)\n\nThis is a very basic example, so classic **metrics** such as **IoU** and **optimizer** such as **Adam** are being used.\n\nTraining and validation **runners** are killer feature of _Segmentation Models_.","metadata":{"id":"hYu0FtN6mRzj"}},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\nclass DeepLabV3Decoder(nn.Sequential):\n    def __init__(self, in_channels, out_channels=256, atrous_rates=(12, 24, 36)):\n        super().__init__(\n            ASPP(in_channels, out_channels, atrous_rates),\n            nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n        )\n        self.out_channels = out_channels\n\n    def forward(self, *features):\n        return super().forward(features[-1])\n    \nclass ASPPConv(nn.Sequential):\n    def __init__(self, in_channels, out_channels, dilation):\n        super().__init__(\n            nn.Conv2d(\n                in_channels,\n                out_channels,\n                kernel_size=3,\n                padding=dilation,\n                dilation=dilation,\n                bias=False,\n            ),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n        )\n\n\nclass ASPPSeparableConv(nn.Sequential):\n    def __init__(self, in_channels, out_channels, dilation):\n        super().__init__(\n            SeparableConv2d(\n                in_channels,\n                out_channels,\n                kernel_size=3,\n                padding=dilation,\n                dilation=dilation,\n                bias=False,\n            ),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n        )\n\n\nclass ASPPPooling(nn.Sequential):\n    def __init__(self, in_channels, out_channels):\n        super().__init__(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n#             nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n        )\n\n    def forward(self, x):\n        size = x.shape[-2:]\n        for mod in self:\n            x = mod(x)\n        return F.interpolate(x, size=size, mode='bilinear', align_corners=False)\n\n\nclass ASPP(nn.Module):\n    def __init__(self, in_channels, out_channels, atrous_rates, separable=False):\n        super(ASPP, self).__init__()\n        modules = []\n        modules.append(\n            nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, 1, bias=False),\n                nn.BatchNorm2d(out_channels),\n                nn.ReLU(),\n            )\n        )\n\n        rate1, rate2, rate3 = tuple(atrous_rates)\n        ASPPConvModule = ASPPConv if not separable else ASPPSeparableConv\n\n        modules.append(ASPPConvModule(in_channels, out_channels, rate1))\n        modules.append(ASPPConvModule(in_channels, out_channels, rate2))\n        modules.append(ASPPConvModule(in_channels, out_channels, rate3))\n        modules.append(ASPPPooling(in_channels, out_channels))\n\n        self.convs = nn.ModuleList(modules)\n\n        self.project = nn.Sequential(\n            nn.Conv2d(5 * out_channels, out_channels, kernel_size=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n        )\n\n    def forward(self, x):\n        res = []\n        for conv in self.convs:\n            res.append(conv(x))\n        res = torch.cat(res, dim=1)\n        return self.project(res)\n\n\nclass SeparableConv2d(nn.Sequential):\n\n    def __init__(\n            self,\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride=1,\n            padding=0,\n            dilation=1,\n            bias=True,\n    ):\n        dephtwise_conv = nn.Conv2d(\n            in_channels,\n            in_channels,\n            kernel_size,\n            stride=stride,\n            padding=padding,\n            dilation=dilation,\n            groups=in_channels,\n            bias=False,\n        )\n        pointwise_conv = nn.Conv2d(\n            in_channels,\n            out_channels,\n            kernel_size=1,\n            bias=bias,\n        )\n        super().__init__(dephtwise_conv, pointwise_conv)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T19:35:16.935730Z","iopub.execute_input":"2021-09-27T19:35:16.937602Z","iopub.status.idle":"2021-09-27T19:35:16.970349Z","shell.execute_reply.started":"2021-09-27T19:35:16.937561Z","shell.execute_reply":"2021-09-27T19:35:16.969600Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"from typing import Optional\nfrom segmentation_models_pytorch.base import SegmentationModel, SegmentationHead, ClassificationHead\nfrom segmentation_models_pytorch.encoders import get_encoder\n\nclass DeepLabV3(SegmentationModel):\n    \"\"\"DeepLabV3_ implementation from \"Rethinking Atrous Convolution for Semantic Image Segmentation\"\n    Args:\n        encoder_name: Name of the classification model that will be used as an encoder (a.k.a backbone)\n            to extract features of different spatial resolution\n        encoder_depth: A number of stages used in encoder in range [3, 5]. Each stage generate features \n            two times smaller in spatial dimensions than previous one (e.g. for depth 0 we will have features\n            with shapes [(N, C, H, W),], for depth 1 - [(N, C, H, W), (N, C, H // 2, W // 2)] and so on).\n            Default is 5\n        encoder_weights: One of **None** (random initialization), **\"imagenet\"** (pre-training on ImageNet) and \n            other pretrained weights (see table with available weights for each encoder_name)\n        decoder_channels: A number of convolution filters in ASPP module. Default is 256\n        in_channels: A number of input channels for the model, default is 3 (RGB images)\n        classes: A number of classes for output mask (or you can think as a number of channels of output mask)\n        activation: An activation function to apply after the final convolution layer.\n            Available options are **\"sigmoid\"**, **\"softmax\"**, **\"logsoftmax\"**, **\"tanh\"**, **\"identity\"**, **callable** and **None**.\n            Default is **None**\n        upsampling: Final upsampling factor. Default is 8 to preserve input-output spatial shape identity\n        aux_params: Dictionary with parameters of the auxiliary output (classification head). Auxiliary output is build \n            on top of encoder if **aux_params** is not **None** (default). Supported params:\n                - classes (int): A number of classes\n                - pooling (str): One of \"max\", \"avg\". Default is \"avg\"\n                - dropout (float): Dropout factor in [0, 1)\n                - activation (str): An activation function to apply \"sigmoid\"/\"softmax\" (could be **None** to return logits)\n    Returns:\n        ``torch.nn.Module``: **DeepLabV3**\n    .. _DeeplabV3:\n        https://arxiv.org/abs/1706.05587\n    \"\"\"\n\n    def __init__(\n            self,\n            encoder_name: str = \"resnet34\",\n            encoder_depth: int = 5,\n            encoder_weights: Optional[str] = \"imagenet\",\n            decoder_channels: int = 256,\n            in_channels: int = 3,\n            classes: int = 1,\n            activation: Optional[str] = None,\n            upsampling: int = 8,\n            aux_params: Optional[dict] = None,\n    ):\n        super().__init__()\n\n        self.encoder = get_encoder(\n            encoder_name,\n            in_channels=in_channels,\n            depth=encoder_depth,\n            weights=encoder_weights,\n            output_stride=8,\n        )\n\n        self.decoder = DeepLabV3Decoder(\n            in_channels=self.encoder.out_channels[-1],\n            out_channels=decoder_channels,\n        )\n\n        self.segmentation_head = SegmentationHead(\n            in_channels=self.decoder.out_channels,\n            out_channels=classes,\n            activation=activation,\n            kernel_size=1,\n            upsampling=upsampling,\n        )\n\n        if aux_params is not None:\n            self.classification_head = ClassificationHead(\n                in_channels=self.encoder.out_channels[-1], **aux_params\n            )\n        else:\n            self.classification_head = None\n","metadata":{"execution":{"iopub.status.busy":"2021-09-27T19:35:16.974462Z","iopub.execute_input":"2021-09-27T19:35:16.976584Z","iopub.status.idle":"2021-09-27T19:35:18.436259Z","shell.execute_reply.started":"2021-09-27T19:35:16.976547Z","shell.execute_reply":"2021-09-27T19:35:18.435551Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\nimport segmentation_models_pytorch as smp\n\n\ndevice = ['cpu', 'cuda'][torch.cuda.is_available()]\nconfig['device'] = device\n\n# Model\nmodel = DeepLabV3(encoder_name=config['model_encoder'].lower(),\n                  encoder_weights=config['model_pretrain'].lower(),\n                  in_channels=config['model_channels'],\n                  classes=len(CLASSES))\n\n# This attribute will not be saved along with model's state_dict\nsetattr(model, 'stage', 0)\n\n# Loss functions\n# loss = smp.utils.losses.DiceLoss()\nmode = smp.losses.MULTICLASS_MODE\n# loss = smp.losses.DiceLoss(mode=mode, classes=[1, 2])\n# setattr(loss, '__name__', 'dice_loss')\nloss = smp.losses.FocalLoss(mode=mode)\nsetattr(loss, '__name__', 'focal_loss')\n\n# Update config\nconfig['model'] = model.__str__().split('(')[0]\nconfig['loss'] = loss.__name__\nconfig['loss_mode'] = mode\n\n# Metrics\nmetrics = [\n           smp.utils.metrics.IoU(threshold=0.5)\n]\n\n# Optimizer\noptimizer = torch.optim.Adam([{\n    'params': model.parameters(),\n    'lr': 2e-4\n}])\n\n# Update config\nconfig['optimizer'] = optimizer.__str__().split()[0]\nconfig['learning_rate'] = optimizer.param_groups[0]['lr']\n\n# Runners\ntrain_one_epoch = smp.utils.train.TrainEpoch(model, loss=loss, metrics=metrics,\n                                             optimizer=optimizer, device=device,\n                                             verbose=False)\nvalid_one_epoch = smp.utils.train.ValidEpoch(model, loss=loss, metrics=metrics,\n                                             device=device, verbose=False)","metadata":{"id":"v9Fz8dW3FfuY","execution":{"iopub.status.busy":"2021-09-27T19:35:18.439000Z","iopub.execute_input":"2021-09-27T19:35:18.439256Z","iopub.status.idle":"2021-09-27T19:35:26.721893Z","shell.execute_reply.started":"2021-09-27T19:35:18.439223Z","shell.execute_reply":"2021-09-27T19:35:26.721170Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## Train loop\n\nAll the models are saved to `PATH_MODELS` under a separate subdirectory with timestamp in its name.\n\nThe best score and the **best** corresponding **weights** are saved after `EPOCHS_MIN` (10th by default) epoch. Final **last weights** are saved at the end (if everything goes well or after `EPOCHS_MIN` epochs if an error occurs).\n\n**Learning rate** is decreased by 5% every epoch.\n\n> `TRAIN=False` allows to skip training and just load a saved model. In order to start training from a scratch, just set `TRAIN=True` and `NAME_PRELOAD=''` (both variables must not evaluate to `False`).","metadata":{"id":"VFhh_g8fmV55"}},{"cell_type":"code","source":"%%time\n\nimport os\n\nfrom datetime import datetime\n\n\nNAME_PRELOAD = ''\nFORMAT_DATE = '%Y-%m-%d-%H-%M-%S'\nEPOCHS_MIN = 10\nEPOCHS = 10\nTRAIN = True\n\ntimestamp = datetime.utcnow().strftime(FORMAT_DATE)\npath_model_best = osp.join(PATH_MODELS, timestamp, 'best.pth')\npath_model_last = osp.join(PATH_MODELS, timestamp, 'last.pth')\npath_model_onnx = osp.join(PATH_MODELS, timestamp, 'model.onnx')\nscore_max = 0\n\nif NAME_PRELOAD:\n    try:\n        path_model_resume = path_model_last.replace(timestamp, NAME_PRELOAD)\n        model = torch.load(path_model_resume, map_location=device)\n        if hasattr(model, 'stage'):\n            model.stage += 1\n        print(f\"The model has been successfully loaded from {NAME_PRELOAD}!\")\n    finally:\n        pass\n\n# Update config\nconfig['epochs'] = EPOCHS\nconfig['epochs_min'] = EPOCHS_MIN\nconfig['timestamp'] = timestamp\nconfig['name_preload'] = NAME_PRELOAD\n\n# W&B setup\nif hasattr(model, 'stage') and model.stage > 0:\n    WANDB_STAGE = f\"Stage{model.stage}\"\nelse:\n    WANDB_STAGE = config['model_pretrain'] if config['model_pretrain'] else 'Zero'\nif 'data_split' in config:\n    name_split = f\"Fold{config['data_split']}\"\nelse:\n    name_split = 'Nofold'\n\nWANDB_ENTITY = 'maritimeai'\nWANDB_PROJECT = 'sea-ice-segmentation'\nWANDB_GROUP = '/'.join([config['model'], config['model_encoder'], WANDB_STAGE])\nWANDB_NAME = '/'.join(['Adam-2e-4', name_split, timestamp] +\n                      (['Debug'] if DEBUG else []))\n\nif 'wandb' in locals() and wandb is not None:\n    experiment = wandb.init(entity=WANDB_ENTITY, config=config,\n                            project=WANDB_PROJECT, group=WANDB_GROUP,\n                            name=WANDB_NAME, notes=timestamp)\n    artifact = wandb.Artifact(WANDB_NAME.replace('/', '.'), type='model',\n                              metadata={'items': dataset_train.items})\nelse:\n    experiment = None\n    artifact = None\n    \n\nif TRAIN:\n    os.makedirs(osp.dirname(path_model_best), exist_ok=True)\n    os.makedirs(osp.dirname(path_model_last), exist_ok=True)\n\n    try:\n        if experiment is not None:\n            experiment.watch(model)\n        for i in range(EPOCHS):\n            print(f\"Epoch = {i:3d}, learning rate =\",\n                f\"{optimizer.param_groups[0]['lr']:0.8f}\")\n\n            logs_train = train_one_epoch.run(loader_train)\n            logs_valid = valid_one_epoch.run(loader_valid)\n\n            if score_max < logs_valid['iou_score'] and i >= EPOCHS_MIN:\n                # Save only after 10 epochs\n                score_max = logs_valid['iou_score']\n                torch.save(model, path_model_best)\n                print(f\"Saved best model with score = {score_max:0.4f}!\")\n\n            # Unconditional model saving\n            torch.save(model, path_model_last)\n            print(f\"Saved latest model with score =\",\n                  f\"{logs_valid['iou_score']:0.4f}!\")\n\n            if experiment is not None:\n                experiment.log({'learning_rate': optimizer.param_groups[0]['lr']},\n                               step=i)\n                experiment.log({f\"{k}/train\": v for k, v in logs_train.items()},\n                               step=i)\n                experiment.log({f\"{k}/valid\": v for k, v in logs_valid.items()},\n                               step=i)\n            optimizer.param_groups[0]['lr'] *= 0.95  # step down\n            print()\n    except KeyboardInterrupt:\n        pass\n    finally:\n        try:\n            artifact.add_file(path_model_best)\n            artifact.add_file(path_model_last)\n        except:\n            pass","metadata":{"id":"8JNtzQ4oqyYi","execution":{"iopub.status.busy":"2021-09-27T19:35:26.722895Z","iopub.execute_input":"2021-09-27T19:35:26.723156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Export to ONNX\n\nExporting a model to _ONNX_ format enables it to be run on any platform optimized for CPU or/and GPU.\n\n> Exporting a model to _ONNX_ format with _PyTorch_ does not require _ONNX_ dependencies.","metadata":{}},{"cell_type":"code","source":"if 'model' in locals() and hasattr(model, 'predict'):\n    os.makedirs(osp.dirname(path_model_onnx), exist_ok=True)\n    model = model.cpu()  # inference on CPU\n\n    # Images to use during export to ONNX\n    images, masks_true = [], []\n    for image, mask_true in loader_valid:\n        images.append(image)\n        masks_true.append(mask_true)\n        break\n\n    # PyTorch prediction for later comparison with ONNX model\n    masks_predict = []\n    for image in images:\n        # masks_predict.append(model.predict(image.to(device)).cpu())\n        masks_predict.append(model.predict(image.cpu()))\n\n    # Make EfficientNet TorchScript-friendly\n#     model.encoder.set_swish(memory_efficient=False)\n\n    # Script and export model to ONNX\n    # without dynamic_axes argument resulting model will have\n    # the same dimension size as input during export\n    torch.onnx.export(model, tuple(image.cpu() for image in images),\n                      path_model_onnx, export_params=True,\n                      opset_version=11,  # do_constant_folding=True,\n                      input_names=['input'], output_names=['output'],\n                      dynamic_axes={}\n                     )\n    try:\n        artifact.add_file(path_model_onnx)\n    except:\n        pass","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It may be ok to clean memory after each training (if the same model is not going to be trained twice).","metadata":{}},{"cell_type":"code","source":"try:\n    del model\nexcept:\n    pass\n\ntry:\n    torch.cuda.empty_cache()\nexcept:\n    pass","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference\n","metadata":{}},{"cell_type":"markdown","source":"## Validation subset\n\nInference on validation subset makes sense in combination with visualization.","metadata":{}},{"cell_type":"markdown","source":"### Inference and visualization (PyTorch)\n\nUse last or best model. After each inference memory is going to be cleaned.","metadata":{}},{"cell_type":"code","source":"VALID = True\n\nif VALID and osp.exists(path_model_last):\n    model_eval = torch.load(path_model_last)\n\n    path_predictions = osp.join(PATH_OUTPUT, timestamp, 'predictions')\n    for name, data in zip(dataset_valid.items, loader_valid):\n        image, mask_true = data\n        name = osp.splitext(name)[0] + '.png'\n        mask_predict = model_eval.predict(image.to(device)).cpu()\n        for i, p, t in zip(image, mask_predict, mask_true):\n            draw_one_row(i.permute(1, 2, 0), p.argmax(0), t.squeeze(0),\n                         output=osp.join(path_predictions, name))\n    try:\n        artifact.add_dir(path_predictions, name='predictions')\n    except:\n        pass\n\ntry:\n    del model_eval\nexcept:\n    pass\n\ntry:\n    torch.cuda.empty_cache()\nexcept:\n    pass","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> IMPORTANT: Finalize W&B initialized process.","metadata":{}},{"cell_type":"code","source":"try:\n    experiment.log_artifact(artifact)\nexcept:\n    pass\n\ntry:\n    experiment.finish()\nexcept:\n    pass","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}