{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","execution_count":null,"source":["# %pip install git+https://github.com/qubvel/segmentation_models.pytorch.git@v0.2.0\n","%pip install segmentation-models-pytorch==0.2.0"],"outputs":[],"metadata":{"id":"2srEi1yx_yfe","execution":{"iopub.status.busy":"2021-09-18T14:17:28.298884Z","iopub.execute_input":"2021-09-18T14:17:28.299333Z","iopub.status.idle":"2021-09-18T14:17:35.176952Z","shell.execute_reply.started":"2021-09-18T14:17:28.299233Z","shell.execute_reply":"2021-09-18T14:17:35.175997Z"},"trusted":true}},{"cell_type":"code","execution_count":null,"source":["!pip uninstall torchtext torchaudio fastai -y"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-09-18T14:17:35.179418Z","iopub.execute_input":"2021-09-18T14:17:35.179720Z","iopub.status.idle":"2021-09-18T14:17:37.014671Z","shell.execute_reply.started":"2021-09-18T14:17:35.179681Z","shell.execute_reply":"2021-09-18T14:17:37.013929Z"},"trusted":true}},{"cell_type":"code","execution_count":null,"source":["!pip install -U torchvision=='0.10.0'"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-09-18T14:17:37.016217Z","iopub.execute_input":"2021-09-18T14:17:37.016480Z","iopub.status.idle":"2021-09-18T14:17:43.916178Z","shell.execute_reply.started":"2021-09-18T14:17:37.016447Z","shell.execute_reply":"2021-09-18T14:17:43.915360Z"},"trusted":true}},{"cell_type":"code","execution_count":null,"source":["import torchvision\n","torchvision.__version__ == '0.10.0+cu102'"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-09-18T14:17:43.919875Z","iopub.execute_input":"2021-09-18T14:17:43.920111Z","iopub.status.idle":"2021-09-18T14:17:44.397624Z","shell.execute_reply.started":"2021-09-18T14:17:43.920085Z","shell.execute_reply":"2021-09-18T14:17:44.396956Z"},"trusted":true}},{"cell_type":"markdown","source":["# Common\n","\n","Common part is a prerequisite for all further parts (data loading, training, inference)."],"metadata":{"id":"e_ZPZcHtaK47"}},{"cell_type":"markdown","source":["## Add dataset and save model weights\n","File -> Add or upload data"],"metadata":{}},{"cell_type":"markdown","source":["## Paths\n","\n","`PATH_OUTPUT` is used to save images (for example, inference on a test subset).\n","\n","`PATH_MODELS` is used to save model weights for later inference.\n","\n","`PATH_UPLOAD` is used to copy model weights for later inference.\n","\n","`PATH_DATASET` is a source of GeoTIFF **images** and **masks** for training and inference (for example, test subset)."],"metadata":{"id":"nSdXi6F2aWMs"}},{"cell_type":"code","execution_count":null,"source":["from os import path as osp\n","import os \n","import shutil\n","\n","NOTEBOOK_NAME = ''\n","\n","PATH_MODELS = osp.join('/', 'kaggle', 'working', 'models')\n","PATH_UPLOAD = osp.join('/', 'kaggle', 'input', NOTEBOOK_NAME, 'models')\n","PATH_DATASET = osp.join('/', 'kaggle', 'input', 'sentinel1', 'dataset')\n","PATH_OUTPUT  = osp.join('/', 'kaggle', 'working', 'output')\n","\n","print('\\n'.join((NOTEBOOK_NAME, PATH_MODELS, PATH_DATASET, PATH_UPLOAD, PATH_OUTPUT)))"],"outputs":[],"metadata":{"id":"e4vFyT2LTzN7","execution":{"iopub.status.busy":"2021-09-18T14:36:29.586452Z","iopub.execute_input":"2021-09-18T14:36:29.587228Z","iopub.status.idle":"2021-09-18T14:36:29.593651Z","shell.execute_reply.started":"2021-09-18T14:36:29.587183Z","shell.execute_reply":"2021-09-18T14:36:29.592928Z"},"trusted":true}},{"cell_type":"code","execution_count":43,"source":["if osp.exists(PATH_UPLOAD) and not osp.exists(PATH_MODELS):\n","    shutil.copytree(PATH_UPLOAD, 'models')\n","elif not osp.exists(PATH_MODELS):\n","    os.mkdir(PATH_MODELS)"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-09-18T14:36:29.707442Z","iopub.execute_input":"2021-09-18T14:36:29.708033Z","iopub.status.idle":"2021-09-18T14:36:29.712906Z","shell.execute_reply.started":"2021-09-18T14:36:29.708002Z","shell.execute_reply":"2021-09-18T14:36:29.711865Z"},"trusted":true}},{"cell_type":"markdown","source":["## Functions\n","\n","Auxiliary functions (for example, drawing/plotting data)."],"metadata":{"id":"-vIWdMcLXMxZ"}},{"cell_type":"code","execution_count":null,"source":["import os\n","\n","from matplotlib import pyplot as plt\n","\n","\n","def draw_one_row(*images, size=1024, output=None):\n","    try:\n","        size = size[:2]\n","    except:\n","        size = (size, size)\n","    count = len(images)\n","    figure, axes = plt.subplots(1, count, dpi=72,\n","                                figsize=(size[0] / 72, size[1] / 72))\n","    for i in range(count):\n","        axes[i].imshow(images[i])\n","    if output is not None:\n","        try:\n","            os.makedirs(osp.dirname(output), exist_ok=True)\n","            plt.savefig(output)\n","        except:\n","            pass\n","    plt.show()"],"outputs":[],"metadata":{"id":"S-CGHBQFXQ44","execution":{"iopub.status.busy":"2021-09-18T14:09:58.326446Z","iopub.execute_input":"2021-09-18T14:09:58.326747Z","iopub.status.idle":"2021-09-18T14:09:58.337235Z","shell.execute_reply.started":"2021-09-18T14:09:58.326718Z","shell.execute_reply":"2021-09-18T14:09:58.335924Z"},"trusted":true}},{"cell_type":"markdown","source":["# Config\n","\n","Configuration dictionary to store training parameters. This config dictionary will be updated in subsequent cells."],"metadata":{"id":"FhMnFHJsUrIb"}},{"cell_type":"code","execution_count":null,"source":["config = {\n","    'classes': ['nodata', 'water', 'ice'],\n","    'batch_size_train': 1,\n","    'batch_size_valid': 1,\n","    'num_workers_train': 1,\n","    'num_workers_valid': 1,\n","    'model_encoder': 'EfficientNet-B2',\n","    'model_pretrain': 'ImageNet',\n","    'model_channels': 3,\n","    'data_split': 2,\n","    'expand': True,\n","    'debug': False,\n","    'flat': True,\n","}"],"outputs":[],"metadata":{"id":"jnRF2Gc_U7Gi","execution":{"iopub.status.busy":"2021-09-18T14:10:02.855055Z","iopub.execute_input":"2021-09-18T14:10:02.855781Z","iopub.status.idle":"2021-09-18T14:10:02.860869Z","shell.execute_reply.started":"2021-09-18T14:10:02.855744Z","shell.execute_reply":"2021-09-18T14:10:02.859969Z"},"trusted":true}},{"cell_type":"markdown","source":["# Data Loading\n"],"metadata":{"id":"y52v0cNRKAs6"}},{"cell_type":"markdown","source":["## Dataset class\n","\n","The most interesting part of the pipeline, 'cause it's an interface between data and the neural network models. Here is an example of a dataset class (`DatasetSAR`) which loads a pair of HH+HV images and their corresponding masks from different directories, stacks them together, applies goemetrics transformations and color augmentations (here the general _augmentations_ term was split into geometric and color in such a way).\n","\n","Parameters for the `DatasetSAR` class constructor:\n","\n","* `paths_images` is a list of strings (or a single string in case of only one polarization used) where each string point to an existing directory with HH or HV polarized images;\n","* `paths_masks` is a list of strings / single string that point(s) to where masks are located (only first directory from the list is used);\n","* `classes` is a list of classes used — important for multiclass masks (where they should be **expanded** into one-hot encoded tensor);\n","* `items` is a list of filenames to be used as the dataset items (WARNING: passed list of filenames is acceted as-is with no checks) — useful for creating subsets (for example, train/valid) from some superset (list of all items of the dataset);\n","* `transformations` is a [torchvision transforms](https://pytorch.org/vision/stable/transforms.html) class (or any other compatible callable class) **without** things like `ToTensor` (it is added separately to the images only) that is applied to both images and masks;\n","* `augmentations` is a callable like `transformations`, but is applied to images only;\n","* `size` is an integer or 2x tuple of integers for final image/mask output size (for example, 1024x1024 as input image for Unet with EfficientNet-B2 encoder);\n","* `expand` is a boolean flag which true value enables expanding a mask into one-hot encoded tensor (that is required for some loss functions and their modes, default is `False` — do not expand);\n","* `flat` is a boolean flag which true value produces squeezed mask (`(H, W)` instead of `(C, H, W)`, for example, for Focal Loss target) — takes precedence over `expand` flag (if `flat` then it's never `expand`ed).\n","\n","> DEBUG: commented line with `np.clip` is for testing binary segmentation.\n","\n","> TODO: add loading and geometric transformations for RGB images."],"metadata":{"id":"iaDLWDQsmbyK"}},{"cell_type":"code","execution_count":null,"source":["from collections import OrderedDict\n","from glob import glob\n","\n","import cv2 as cv\n","import numpy as np\n","import torch\n","\n","from PIL import Image\n","from torch.utils.data import Dataset\n","from torchvision.transforms import Compose, Lambda, RandomResizedCrop, Resize, ToTensor\n","from torchvision.transforms import InterpolationMode\n","\n","\n","class DatasetSAR(Dataset):\n","    def __init__(self, paths_images, paths_masks, classes, items=None,\n","                 transformations=None, augmentations=None, size=None,\n","                 expand=False, flat=False):\n","        # Assert images paths\n","        if isinstance(paths_images, str):\n","            paths_images = (paths_images, path_images)\n","        elif not isinstance(paths_images, (tuple, list, set)):\n","            raise TypeError(\"first argument must be of type str or list!\")\n","        else:\n","            paths_images = tuple(paths_images)\n","        # Assert masks paths\n","        if isinstance(paths_masks, str):\n","            paths_masks = (paths_masks,)\n","        elif not isinstance(paths_masks, (tuple, list, set)):\n","            raise TypeError(\"second argument must be of type str or list!\")\n","        else:\n","            paths_masks = tuple(paths_masks)\n","        # Default output image/mask size\n","        if size is None:\n","            size = (1024, 1024)\n","        # Default transformations (geometric - image + mask)\n","        mode_interpolation = InterpolationMode.NEAREST\n","        if not isinstance(transformations, (Compose, torch.nn.Module)):\n","            self.transformations = Compose([\n","                Resize(size, mode_interpolation),\n","            ])\n","        else:\n","            self.transformations = transformations\n","        # Default augmentations (color - image only)\n","        if not isinstance(augmentations, (Compose, torch.nn.Module)):\n","            self.augmentations = Compose([\n","                Lambda(lambda x: x),\n","            ])\n","        else:\n","            self.augmentations = augmentations\n","        # Class attributes\n","        self.to_tensor = ToTensor()\n","        self.paths_images = paths_images\n","        self.paths_masks = paths_masks\n","        if items is None:\n","            items = []\n","            for path in paths_images[:2] + paths_masks[:1]:\n","                items.append({osp.basename(item) for item \\\n","                            in glob(osp.join(path, '*.tiff'))})\n","            self.items = sorted(items[0].intersection(*items))\n","        else:\n","            self.items = items\n","        self.classes = len(classes)\n","        self.items_class = OrderedDict({c: i for c, i \\\n","                                        in zip(classes,\n","                                               range(1, self.classes + 1))})\n","        self.expand = expand\n","        self.flat = flat\n","        return None\n","\n","    def __getitem__(self, item):\n","        # TODO: load RGBs\n","        image_hh = cv.imread(osp.join(self.paths_images[0], self.items[item]),\n","                             cv.IMREAD_LOAD_GDAL | cv.IMREAD_GRAYSCALE)\n","        h, w = image_hh.shape[:2]\n","        image_hv = cv.imread(osp.join(self.paths_images[1], self.items[item]),\n","                             cv.IMREAD_LOAD_GDAL | cv.IMREAD_GRAYSCALE)\n","        h, w = tuple(map(min, zip(image_hv.shape[:2], (h, w))))\n","        if self.paths_masks:\n","            mask = cv.imread(osp.join(self.paths_masks[0], self.items[item]),\n","                            cv.IMREAD_LOAD_GDAL | cv.IMREAD_GRAYSCALE)\n","        else:\n","            mask = image_hh\n","        h, w = tuple(map(min, zip(mask.shape[:2], (h, w))))\n","        # Use minimum height and width 'cause image/mask dimension may not\n","        # always fit (difference during mask conversion)\n","        image = Image.fromarray(np.dstack(\n","            [image_hh[:h, :w], image_hv[:h, :w], mask[:h, :w]]\n","            # [image_hh[:h, :w], image_hv[:h, :w], np.clip(mask[:h, :w], 0, 1)]\n","        ))\n","        del image_hh, image_hv, mask\n","        image = self.transformations(image)\n","        image = np.array(image)\n","        image, mask = image[..., :2], image[..., 2]\n","        image = self.to_tensor(np.dstack((image[..., 1], image)))\n","        image = self.augmentations(image)\n","        # Convert to int64 'cause OHE requires index tensor\n","        if self.flat:\n","            mask = torch.tensor(mask)\n","        elif self.expand:\n","            mask = torch.nn.functional.one_hot(torch.tensor(mask,\n","                                                            dtype=torch.int64),\n","                                               self.classes).to(torch.int8)\\\n","                                               .permute(2, 0, 1)\n","        else:\n","            mask = torch.tensor(mask).unsqueeze_(0).to(torch.int64)\n","        return image, mask\n","\n","    def __len__(self):\n","        return len(self.items)\n","\n","    def debug(self):\n","        # Debug function to show some dataset stats\n","        print(f\"DEBUG: paths images = {self.paths_images}\")\n","        print(f\"DEBUG: paths masks = {self.paths_masks}\")\n","        print(f\"DEBUG: items ({len(self.items)}):\")\n","        print('\\n'.join(self.items))\n","        print(f\"DEBUG: classes ({self.classes}):\")\n","        print(self.items_class)"],"outputs":[],"metadata":{"id":"EHVKXKfwKF2d","execution":{"iopub.status.busy":"2021-09-18T14:10:05.744701Z","iopub.execute_input":"2021-09-18T14:10:05.744991Z","iopub.status.idle":"2021-09-18T14:10:05.770743Z","shell.execute_reply.started":"2021-09-18T14:10:05.744962Z","shell.execute_reply":"2021-09-18T14:10:05.769812Z"},"trusted":true}},{"cell_type":"markdown","source":["## Datasets and visualization\n","\n","This part produces datasets to be used in data loaders for training and validation. Here are three datasets: `dataset` — grand superset with all annotated images; `dataset_train` — training subset of grand dataset to be used during training (calculating gradients); `dataset_valid` — validation subset of grand dataset to be used during validation (calculating metrics).\n","\n","> Training/validation items (`items_train`/`items_valid`) are selected evenly across the grand dataset items (feel free to split it in any other way).\n","\n","Transformations are also in two variants (train/valid): **random crop and resize** for training in order to learn features from different scales and simple **resize** for validation (for simplicity and closer approach to later inference scenarios).\n","\n","> All transformations suppose output shape of images/masks to be 1024x1024 (set by `size` valiable).\n","\n","Images and masks from train/valid dataset classes are visualized in the last two loops. As for _MaritimeAI Sentinel-1 Dataset 1920_, there will be displayed 124 images and corresponding masks (neither too much nor too little).\n","\n","Variables `EXPAND` and `FLAT` are used for top-level control of mask output format. For example, for Dice loss from _Segmentation Models_ _utils_ module set `EXPAND=True` and `FLAT=False`, for Dice loss from _Segmentation Models_ _losses_ module set all two variables `False`, for Focal loss from _losses_ module set `FLAT=True`.\n","\n","Variable `DEBUG` sets debug mode for overfitting on one image.\n","\n","Class list used for two-class segmentation (from _MaritimeAI Sentinel-1 Dataset 1920_ `2-class` directory) includes: `nodata` (zero values), `water` (ones) and `ice` (twos).\n","\n","> Formally, there are only `water` and `ice` classes of interest, but technically, all three classes must be specified.\n","\n","> DEBUG: for true binary segmentation there should be one class (for example, `data`).\n","\n","> Yet another variable `NUM_SAMPLES` had been added to control the number of output samples from training and validation datasets (preview data loading transformations and augmentations)."],"metadata":{"id":"4fHWXrItk1CS"}},{"cell_type":"code","execution_count":null,"source":["items_exclude_a = [\n","    'S1B_EW_GRDM_1SDH_20200203T031613_20200203T031631_020099_0260A6_D03B.tiff',\n","    'S1B_EW_GRDM_1SDH_20200215T031613_20200215T031630_020274_026647_9E25.tiff',\n","    'S1B_EW_GRDM_1SDH_20200227T031613_20200227T031630_020449_026BE9_3282.tiff',\n","    'S1B_EW_GRDM_1SDH_20200310T031613_20200310T031630_020624_027178_1A36.tiff',\n","    'S1B_EW_GRDM_1SDH_20200322T031613_20200322T031631_020799_027702_664C.tiff',\n","    'S1B_EW_GRDM_1SDH_20200521T031615_20200521T031633_021674_029249_923C.tiff',\n","]\n","\n","items_exclude_b = [\n","    'S1A_EW_GRDM_1SDH_20191117T031700_20191117T031800_029945_036ADD_32F2.tiff',\n","    'S1A_EW_GRDM_1SDH_20191129T031659_20191129T031759_030120_0370EF_D07E.tiff',\n","    'S1A_EW_GRDM_1SDH_20200104T031658_20200104T031758_030645_038306_DDA1.tiff',\n","    'S1A_EW_GRDM_1SDH_20200328T031656_20200328T031756_031870_03ADB5_D992.tiff',\n","    'S1A_EW_GRDM_1SDH_20200421T031657_20200421T031757_032220_03BA08_1F43.tiff',\n","]\n","\n","items_exclude_c = [\n","    'S1A_EW_GRDM_1SDH_20191211T031659_20191211T031759_030295_0376F4_BE3E.tiff',\n","    'S1A_EW_GRDM_1SDH_20191223T031658_20191223T031758_030470_037CFD_AB38.tiff',\n","    'S1A_EW_GRDM_1SDH_20200221T031656_20200221T031756_031345_039B6D_927B.tiff',\n","    'S1A_EW_GRDM_1SDH_20200304T031656_20200304T031756_031520_03A17D_08EB.tiff',\n","    'S1A_EW_GRDM_1SDH_20200316T031656_20200316T031756_031695_03A78C_D3A3.tiff',\n","    'S1A_EW_GRDM_1SDH_20200409T031657_20200409T031757_032045_03B3E6_7A01.tiff',\n","    'S1A_EW_GRDM_1SDH_20200503T031658_20200503T031758_032395_03C031_950B.tiff',\n","]\n","\n","items_exclude_d = [\n","    'S1A_EW_GRDM_1SDH_20191107T030034_20191107T030132_029799_0365BB_F7CF.tiff',\n","    'S1B_EW_GRDM_1SDH_20200601T023525_20200601T023601_021834_02971A_B08C.tiff'\n","]"],"outputs":[],"metadata":{"id":"lUcO6jcM7jzf","execution":{"iopub.status.busy":"2021-09-18T14:10:08.025465Z","iopub.execute_input":"2021-09-18T14:10:08.025735Z","iopub.status.idle":"2021-09-18T14:10:08.031672Z","shell.execute_reply.started":"2021-09-18T14:10:08.025709Z","shell.execute_reply":"2021-09-18T14:10:08.030623Z"},"trusted":true}},{"cell_type":"code","execution_count":null,"source":["from random import sample\n","\n","\n","CLASSES = config['classes']\n","# CLASSES = ['data']  # do np.clip(mask, 0, 1) for binary mode\n","NUM_SAMPLES = 6  # draw samples from train/valid datasets\n","DATA_SPLIT = config['data_split'] - 1 if 'data_split' in config else None\n","\n","DEBUG = config['debug']\n","EXPAND = config['expand']\n","FLAT = config['flat']\n","\n","# Nearest interpolation mode is mandatory for masks\n","# and optional (but recommended) for images\n","mode_interpolation = InterpolationMode.NEAREST\n","\n","# Separate transformations for train and valid\n","size = (1024, 1024)  # size for network input\n","transformations_train = Compose([\n","    RandomResizedCrop(size, (0.25, 0.95), (3 / 4, 4 / 3),\n","                      mode_interpolation),\n","])\n","\n","transformations_valid = Compose([\n","    Resize(size, mode_interpolation),\n","])\n","\n","# Images/masks paths\n","path_images_hh = osp.join(PATH_DATASET, 'images', 'hh')\n","path_images_hv = osp.join(PATH_DATASET, 'images', 'hv')\n","path_masks = osp.join(PATH_DATASET, 'masks', '2-class')\n","\n","# Grand dataset\n","dataset = DatasetSAR((path_images_hh, path_images_hv), path_masks,\n","                     classes=CLASSES)\n","\n","# Split the dataset into train/valid datasets\n","items_total = len(dataset)\n","fraction = 100 / 15\n","\n","# Train/valid items (images and masks)\n","if DATA_SPLIT is None:\n","    items_train = tuple(dataset.items[i] for i in range(items_total) \\\n","                        if not i or i % int(round(fraction)))\n","\n","    items_valid = tuple(dataset.items[i] for i in range(items_total) \\\n","                        if i and not i % int(round(fraction)))\n","else:\n","    items_all = [item for item in sorted(dataset.items) if item not in \n","                 items_exclude_a + items_exclude_b + items_exclude_c +\n","                 items_exclude_d]\n","    items_split = [{\n","        'train': sorted(set(items_all) - set(items_all[i::5])),\n","        'valid': sorted(items_all[i::5])\n","    } for i in range(5)]\n","    items_train = items_split[DATA_SPLIT]['train']\n","    items_valid = items_split[DATA_SPLIT]['valid']\n","\n","# Debug: set default transformations for training and validation\n","if DEBUG:\n","    transformations_train = None\n","    transformations_valid = None\n","\n","dataset_train = DatasetSAR((path_images_hh, path_images_hv), path_masks,\n","                           items=items_train, classes=CLASSES, expand=EXPAND,\n","                           flat=FLAT, transformations=transformations_train)\n","\n","dataset_valid = DatasetSAR((path_images_hh, path_images_hv), path_masks,\n","                           items=items_valid, classes=CLASSES, expand=EXPAND,\n","                           flat=FLAT, transformations=transformations_valid)\n","\n","# Debug: make datasets of one item\n","if DEBUG:\n","    dataset_train.items = dataset.items[:1]\n","    dataset_valid.items = dataset.items[:1]\n","\n","nomasks = []\n","size_train = len(dataset_train)\n","print(f\"Training dataset ({size_train}):\")\n","for i in sample(range(size_train), k=min(NUM_SAMPLES, size_train)):\n","    try:\n","        image, mask = dataset_train[i]\n","        draw_one_row(image.permute(1, 2, 0), mask if FLAT else mask.argmax(0) \\\n","                     if EXPAND else mask.squeeze(0) if not FLAT else mask)\n","    except AttributeError:\n","        nomask = dataset_train.items[i]\n","        nomasks.append(nomask)\n","        print(f\"ERROR: failed to load {nomask}\")\n","print(f\"Read errors = {len(nomasks)}\")\n","\n","config['items_train'] = dataset_train.items\n","config['errors_train'] = nomasks\n","config['size_train'] = size_train\n","\n","nomasks = []\n","size_valid = len(dataset_valid)\n","print(f\"Validation dataset ({size_valid}):\")\n","for i in sample(range(size_valid), k=min(NUM_SAMPLES, size_valid)):\n","    try:\n","        image, mask = dataset_valid[i]\n","        draw_one_row(image.permute(1, 2, 0), mask if FLAT else mask.argmax(0) \\\n","                     if EXPAND else mask.squeeze(0))\n","    except AttributeError:\n","        nomask = dataset_valid.items[i]\n","        nomasks.append(nomask)\n","        print(f\"ERROR: failed to load {nomask}\")\n","print(f\"Read errors = {len(nomasks)}\")\n","\n","config['items_valid'] = dataset_valid.items\n","config['errors_valid'] = nomasks\n","config['size_valid'] = size_valid"],"outputs":[],"metadata":{"id":"qXZSfhywqlGY","execution":{"iopub.status.busy":"2021-09-18T14:10:09.411817Z","iopub.execute_input":"2021-09-18T14:10:09.412382Z","iopub.status.idle":"2021-09-18T14:10:14.434261Z","shell.execute_reply.started":"2021-09-18T14:10:09.412343Z","shell.execute_reply":"2021-09-18T14:10:14.432131Z"},"trusted":true}},{"cell_type":"markdown","source":["`DatasetSAR` class has optional `debug` method to show filenames of items and some other info."],"metadata":{"id":"nQvSiMDNy7zJ"}},{"cell_type":"code","execution_count":null,"source":["# dataset.debug()"],"outputs":[],"metadata":{"id":"hWXaIVDeraup","execution":{"iopub.status.busy":"2021-09-18T14:10:14.436161Z","iopub.execute_input":"2021-09-18T14:10:14.436421Z","iopub.status.idle":"2021-09-18T14:10:14.440987Z","shell.execute_reply.started":"2021-09-18T14:10:14.436388Z","shell.execute_reply":"2021-09-18T14:10:14.440176Z"},"trusted":true}},{"cell_type":"markdown","source":["## Dataloaders\n","\n","_Google Colab_ settings. Batch size can be increased, also number of workers, if neccessary."],"metadata":{"id":"75EX9_pIqPo8"}},{"cell_type":"code","execution_count":null,"source":["from torch.utils.data import DataLoader\n","\n","\n","loader_train = DataLoader(dataset_train, batch_size=config['batch_size_train'],\n","                          shuffle=True, num_workers=config['num_workers_train'])\n","\n","loader_valid = DataLoader(dataset_valid, batch_size=config['batch_size_valid'],\n","                          shuffle=False, num_workers=config['num_workers_valid'])"],"outputs":[],"metadata":{"id":"eLyc1dHhmHZO","execution":{"iopub.status.busy":"2021-09-18T14:10:14.442557Z","iopub.execute_input":"2021-09-18T14:10:14.442874Z","iopub.status.idle":"2021-09-18T14:10:14.454373Z","shell.execute_reply.started":"2021-09-18T14:10:14.442841Z","shell.execute_reply":"2021-09-18T14:10:14.453379Z"},"trusted":true}},{"cell_type":"markdown","source":["# Training"],"metadata":{"id":"_wTOlKWdKDc1"}},{"cell_type":"markdown","source":["## W&B\n","\n","An API token is required in order to authorize in WandB:\n","https://wandb.ai/authorize"],"metadata":{"id":"nux0P43rH6qN"}},{"cell_type":"code","execution_count":null,"source":["%pip install --upgrade wandb"],"outputs":[],"metadata":{"id":"nWczlMT8H58m","execution":{"iopub.status.busy":"2021-09-18T14:10:17.106547Z","iopub.execute_input":"2021-09-18T14:10:17.107383Z","iopub.status.idle":"2021-09-18T14:10:27.369286Z","shell.execute_reply.started":"2021-09-18T14:10:17.107336Z","shell.execute_reply":"2021-09-18T14:10:27.368471Z"},"trusted":true}},{"cell_type":"markdown","source":["Add-ons -> secrets -> new secrets -> label('wandb') -> value(https://wandb.ai/authorize)"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["import wandb\n","from kaggle_secrets import UserSecretsClient\n","\n","user_secrets = UserSecretsClient()\n","secret_value_0 = user_secrets.get_secret(\"wandb\")\n","\n","wandb.login(key=secret_value_0)"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-09-18T14:11:42.819485Z","iopub.execute_input":"2021-09-18T14:11:42.81982Z","iopub.status.idle":"2021-09-18T14:11:45.416877Z","shell.execute_reply.started":"2021-09-18T14:11:42.819776Z","shell.execute_reply":"2021-09-18T14:11:45.416165Z"},"trusted":true}},{"cell_type":"markdown","source":["## Model\n","\n","**Unet** with **EfficientNet-B2** encoder is used as an example 'cause it does fit into _Google Colab_ GPU RAM. EfficientNet-B3 may also fit (larger encoders like EfficientNet-B7 were tested on local devboxes and gave the best results).\n","\n","For some losses from _Segmentation Models_ it may require to change mask output format (squeezed or one-hot expanded, see `EXPAND` and `FLAT` variables for datasets).\n","\n","Experimenting with **loss functions** may give interesting results. Compare the same scenario with different loss functions:\n","\n","* DiceLoss (with classes 1 and 2)\n","\n","![DiceLoss](https://raw.githubusercontent.com/MaritimeAI/ODS-SoC/1b066dd63aedd364fda4ad0d08ac3ae6286f2289/assets/images/dice_12.png)\n","\n","* FocalLoss\n","\n","![FocalLoss](https://raw.githubusercontent.com/MaritimeAI/ODS-SoC/1b066dd63aedd364fda4ad0d08ac3ae6286f2289/assets/images/focal_all.png)\n","\n","This is a very basic example, so classic **metrics** such as **IoU** and **optimizer** such as **Adam** are being used.\n","\n","Training and validation **runners** are killer feature of _Segmentation Models_."],"metadata":{"id":"hYu0FtN6mRzj"}},{"cell_type":"code","execution_count":null,"source":["import segmentation_models_pytorch as smp\n","\n","\n","device = ['cpu', 'cuda'][torch.cuda.is_available()]\n","config['device'] = device\n","\n","# Model\n","model = smp.Unet(encoder_name=config['model_encoder'].lower(),\n","                 encoder_weights=config['model_pretrain'].lower(),\n","                 in_channels=config['model_channels'],\n","                 classes=len(CLASSES))\n","\n","# This attribute will not be saved along with model's state_dict\n","setattr(model, 'stage', 0)\n","\n","# Loss functions\n","# loss = smp.utils.losses.DiceLoss()\n","mode = smp.losses.MULTICLASS_MODE\n","# loss = smp.losses.DiceLoss(mode=mode, classes=[1, 2])\n","# setattr(loss, '__name__', 'dice_loss')\n","loss = smp.losses.FocalLoss(mode=mode)\n","setattr(loss, '__name__', 'focal_loss')\n","\n","# Update config\n","config['model'] = model.__str__().split('(')[0]\n","config['loss'] = loss.__name__\n","config['loss_mode'] = mode\n","\n","# Metrics\n","metrics = [\n","           smp.utils.metrics.IoU(threshold=0.5)\n","]\n","\n","# Optimizer\n","optimizer = torch.optim.Adam([{\n","    'params': model.parameters(),\n","    'lr': 2e-4\n","}])\n","\n","# Update config\n","config['optimizer'] = optimizer.__str__().split()[0]\n","config['learning_rate'] = optimizer.param_groups[0]['lr']\n","\n","# Runners\n","train_one_epoch = smp.utils.train.TrainEpoch(model, loss=loss, metrics=metrics,\n","                                             optimizer=optimizer, device=device,\n","                                             verbose=False)\n","valid_one_epoch = smp.utils.train.ValidEpoch(model, loss=loss, metrics=metrics,\n","                                             device=device, verbose=False)"],"outputs":[],"metadata":{"id":"v9Fz8dW3FfuY","execution":{"iopub.status.busy":"2021-09-18T14:12:17.1824Z","iopub.execute_input":"2021-09-18T14:12:17.182684Z","iopub.status.idle":"2021-09-18T14:12:22.810054Z","shell.execute_reply.started":"2021-09-18T14:12:17.182652Z","shell.execute_reply":"2021-09-18T14:12:22.809282Z"},"trusted":true}},{"cell_type":"markdown","source":["## Train loop\n","\n","All the models are saved to `PATH_MODELS` under a separate subdirectory with timestamp in its name.\n","\n","The best score and the **best** corresponding **weights** are saved after `EPOCHS_MIN` (10th by default) epoch. Final **last weights** are saved at the end (if everything goes well or after `EPOCHS_MIN` epochs if an error occurs).\n","\n","**Learning rate** is decreased by 5% every epoch.\n","\n","> `TRAIN=False` allows to skip training and just load a saved model. In order to start training from a scratch, just set `TRAIN=True` and `NAME_PRELOAD=''` (both variables must not evaluate to `False`)."],"metadata":{"id":"VFhh_g8fmV55"}},{"cell_type":"code","execution_count":null,"source":["%%time\n","\n","import os\n","\n","from datetime import datetime\n","\n","\n","NAME_PRELOAD = ''\n","FORMAT_DATE = '%Y-%m-%d-%H-%M-%S'\n","EPOCHS_MIN = 10\n","EPOCHS = 30\n","TRAIN = True\n","\n","timestamp = datetime.utcnow().strftime(FORMAT_DATE)\n","path_model_best = osp.join(PATH_MODELS, timestamp, 'best.pth')\n","path_model_last = osp.join(PATH_MODELS, timestamp, 'last.pth')\n","path_model_onnx = osp.join(PATH_MODELS, timestamp, 'model.onnx')\n","score_max = 0\n","\n","if NAME_PRELOAD:\n","    try:\n","        path_model_resume = path_model_last.replace(timestamp, NAME_PRELOAD)\n","        model = torch.load(path_model_resume, map_location=device)\n","        if hasattr(model, 'stage'):\n","            model.stage += 1\n","        print(f\"The model has been successfully loaded from {NAME_PRELOAD}!\")\n","    finally:\n","        pass\n","\n","# Update config\n","config['epochs'] = EPOCHS\n","config['epochs_min'] = EPOCHS_MIN\n","config['timestamp'] = timestamp\n","config['name_preload'] = NAME_PRELOAD\n","\n","# W&B setup\n","if hasattr(model, 'stage') and model.stage > 0:\n","    WANDB_STAGE = f\"Stage{model.stage}\"\n","else:\n","    WANDB_STAGE = config['model_pretrain'] if config['model_pretrain'] else 'Zero'\n","if 'data_split' in config:\n","    name_split = f\"Fold{config['data_split']}\"\n","else:\n","    name_split = 'Nofold'\n","\n","WANDB_ENTITY = 'maritimeai'\n","WANDB_PROJECT = 'sea-ice-segmentation'\n","WANDB_GROUP = '/'.join([config['model'], config['model_encoder'], WANDB_STAGE])\n","WANDB_NAME = '/'.join(['Baseline', name_split, timestamp] +\n","                      (['Debug'] if DEBUG else []))\n","\n","if 'wandb' in locals() and wandb is not None:\n","    experiment = wandb.init(entity=WANDB_ENTITY, config=config,\n","                            project=WANDB_PROJECT, group=WANDB_GROUP,\n","                            name=WANDB_NAME, notes=timestamp)\n","    artifact = wandb.Artifact(WANDB_NAME.replace('/', '.'), type='model',\n","                              metadata={'items': dataset_train.items})\n","else:\n","    experiment = None\n","    artifact = None\n","    \n","\n","if TRAIN:\n","    os.makedirs(osp.dirname(path_model_best), exist_ok=True)\n","    os.makedirs(osp.dirname(path_model_last), exist_ok=True)\n","\n","    try:\n","        if experiment is not None:\n","            experiment.watch(model)\n","        for i in range(EPOCHS):\n","            print(f\"Epoch = {i:3d}, learning rate =\",\n","                f\"{optimizer.param_groups[0]['lr']:0.8f}\")\n","\n","            logs_train = train_one_epoch.run(loader_train)\n","            logs_valid = valid_one_epoch.run(loader_valid)\n","\n","            if score_max < logs_valid['iou_score'] and i >= EPOCHS_MIN:\n","                # Save only after 10 epochs\n","                score_max = logs_valid['iou_score']\n","                torch.save(model, path_model_best)\n","                print(f\"Saved best model with score = {score_max:0.4f}!\")\n","\n","            # Unconditional model saving\n","            torch.save(model, path_model_last)\n","            print(f\"Saved latest model with score =\",\n","                  f\"{logs_valid['iou_score']:0.4f}!\")\n","\n","            if experiment is not None:\n","                experiment.log({'learning_rate': optimizer.param_groups[0]['lr']},\n","                               step=i)\n","                experiment.log({f\"{k}/train\": v for k, v in logs_train.items()},\n","                               step=i)\n","                experiment.log({f\"{k}/valid\": v for k, v in logs_valid.items()},\n","                               step=i)\n","            optimizer.param_groups[0]['lr'] *= 0.95  # step down\n","            print()\n","    except KeyboardInterrupt:\n","        pass\n","    finally:\n","        try:\n","            artifact.add_file(path_model_best)\n","            artifact.add_file(path_model_last)\n","        except:\n","            pass"],"outputs":[],"metadata":{"id":"8JNtzQ4oqyYi","execution":{"iopub.status.busy":"2021-09-18T14:12:33.854881Z","iopub.execute_input":"2021-09-18T14:12:33.855181Z","iopub.status.idle":"2021-09-18T14:13:22.549754Z","shell.execute_reply.started":"2021-09-18T14:12:33.855149Z","shell.execute_reply":"2021-09-18T14:13:22.54893Z"},"trusted":true}},{"cell_type":"markdown","source":["## Export to ONNX\n","\n","Exporting a model to _ONNX_ format enables it to be run on any platform optimized for CPU or/and GPU.\n","\n","> Exporting a model to _ONNX_ format with _PyTorch_ does not require _ONNX_ dependencies."],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["if 'model' in locals() and hasattr(model, 'predict'):\n","    os.makedirs(osp.dirname(path_model_onnx), exist_ok=True)\n","    model = model.cpu()  # inference on CPU\n","\n","    # Images to use during export to ONNX\n","    images, masks_true = [], []\n","    for image, mask_true in loader_valid:\n","        images.append(image)\n","        masks_true.append(mask_true)\n","        break\n","\n","    # PyTorch prediction for later comparison with ONNX model\n","    masks_predict = []\n","    for image in images:\n","        # masks_predict.append(model.predict(image.to(device)).cpu())\n","        masks_predict.append(model.predict(image.cpu()))\n","\n","    # Make EfficientNet TorchScript-friendly\n","    model.encoder.set_swish(memory_efficient=False)\n","\n","    # Script and export model to ONNX\n","    # without dynamic_axes argument resulting model will have\n","    # the same dimension size as input during export\n","    torch.onnx.export(model, tuple(image.cpu() for image in images),\n","                      path_model_onnx, export_params=True,\n","                      opset_version=11,  # do_constant_folding=True,\n","                      input_names=['input'], output_names=['output'],\n","                      dynamic_axes={}\n","                     )\n","    try:\n","        artifact.add_file(path_model_onnx)\n","    except:\n","        pass"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-09-18T14:13:43.854867Z","iopub.execute_input":"2021-09-18T14:13:43.855171Z","iopub.status.idle":"2021-09-18T14:14:04.301688Z","shell.execute_reply.started":"2021-09-18T14:13:43.855136Z","shell.execute_reply":"2021-09-18T14:14:04.300896Z"},"trusted":true}},{"cell_type":"markdown","source":["It may be ok to clean memory after each training (if the same model is not going to be trained twice)."],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["try:\n","    del model\n","except:\n","    pass\n","\n","try:\n","    torch.cuda.empty_cache()\n","except:\n","    pass"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-09-18T14:15:11.434681Z","iopub.execute_input":"2021-09-18T14:15:11.435039Z","iopub.status.idle":"2021-09-18T14:15:11.606347Z","shell.execute_reply.started":"2021-09-18T14:15:11.434991Z","shell.execute_reply":"2021-09-18T14:15:11.605412Z"},"trusted":true}},{"cell_type":"markdown","source":["# Inference\n"],"metadata":{}},{"cell_type":"markdown","source":["## Validation subset\n","\n","Inference on validation subset makes sense in combination with visualization."],"metadata":{}},{"cell_type":"markdown","source":["### Inference and visualization (PyTorch)\n","\n","Use last or best model. After each inference memory is going to be cleaned."],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["VALID = True\n","\n","if VALID and osp.exists(path_model_last):\n","    model_eval = torch.load(path_model_last)\n","\n","    path_predictions = osp.join(PATH_OUTPUT, timestamp, 'predictions')\n","    for name, data in zip(dataset_valid.items, loader_valid):\n","        image, mask_true = data\n","        name = osp.splitext(name)[0] + '.png'\n","        mask_predict = model_eval.predict(image.to(device)).cpu()\n","        for i, p, t in zip(image, mask_predict, mask_true):\n","            draw_one_row(i.permute(1, 2, 0), p.argmax(0), t.squeeze(0),\n","                         output=osp.join(path_predictions, name))\n","    try:\n","        artifact.add_dir(path_predictions, name='predictions')\n","    except:\n","        pass\n","\n","try:\n","    del model_eval\n","except:\n","    pass\n","\n","try:\n","    torch.cuda.empty_cache()\n","except:\n","    pass"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-09-18T14:15:13.690835Z","iopub.execute_input":"2021-09-18T14:15:13.691558Z","iopub.status.idle":"2021-09-18T14:15:17.711409Z","shell.execute_reply.started":"2021-09-18T14:15:13.691519Z","shell.execute_reply":"2021-09-18T14:15:17.710647Z"},"trusted":true}},{"cell_type":"markdown","source":["> IMPORTANT: Finalize W&B initialized process."],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["try:\n","    experiment.log_artifact(artifact)\n","except:\n","    pass\n","\n","try:\n","    experiment.finish()\n","except:\n","    pass"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-09-18T14:15:17.713398Z","iopub.execute_input":"2021-09-18T14:15:17.713654Z","iopub.status.idle":"2021-09-18T14:15:25.782826Z","shell.execute_reply.started":"2021-09-18T14:15:17.713621Z","shell.execute_reply":"2021-09-18T14:15:25.78193Z"},"trusted":true}}]}