{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "segmentation.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3PJO2BJOOem"
      },
      "source": [
        "# Dependencies\n",
        "\n",
        "Use extra UbuntuGIS repository to get _GDAL_ version 3.0.4 or higher, since _Colab_'s native version 2.2.x is too old for the pipeline.\n",
        "\n",
        "> If after installation version of _GDAL_ at the end is still 2.2.x, then restart runtime."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxlMBnn3doy8"
      },
      "source": [
        "# Check container OS version (for correct UbuntuGIS package version)\n",
        "!lsb_release -a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ze9MPcf1Qi8m"
      },
      "source": [
        "## GDAL\n",
        "\n",
        "> If _GDAL 3.0.4_ (library and _Python_ bindings) or above is already installed in the system, then just skip or comment out the cell below, 'cause it's intended for _Google Colab_, which has _GDAL 2.2.x_ only."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v08tDkPxM8X_"
      },
      "source": [
        "# Dark magic happens here: installing dependencies for GDAL 3.0.4\n",
        "# build process via APT and install GDAL itself via PyPI\n",
        "!add-apt-repository -y ppa:ubuntugis/ubuntugis-unstable\n",
        "!apt install python3-gdal=3.0.4+dfsg-1~bionic0\n",
        "!apt purge --autoremove python3-gdal\n",
        "!pip install gdal==3.0.4\n",
        "!apt install gdal-bin=3.0.4+dfsg-1~bionic0\n",
        "\n",
        "from osgeo import gdal; print(f\"GDAL version {(gdal.__version__)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dF7J0KdFYHS8"
      },
      "source": [
        "## Segmentation models\n",
        "\n",
        "![Segmentation models](https://camo.githubusercontent.com/51eea85ed59f27be0485cc5774d09b522ea8e77cd3f0753c085cacd18d4a41a0/68747470733a2f2f692e6962622e636f2f4774784753386d2f5365676d656e746174696f6e2d4d6f64656c732d56312d536964652d332d312e706e67)\n",
        "\n",
        "_Segmentation models_, _PyTorch_ variant will be used in this pipeline. _Catalyst_ or _PyTorch Lightning_  (or any other deep learning framework) may also be used (but now it's time for _Segmentation Models_).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2srEi1yx_yfe"
      },
      "source": [
        "# %pip install git+https://github.com/qubvel/segmentation_models.pytorch.git@v0.2.0\n",
        "%pip install segmentation-models-pytorch==0.2.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_ZPZcHtaK47"
      },
      "source": [
        "# Common\n",
        "\n",
        "Common part is a prerequisite for all further parts (data loading, training, inference)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cb8uzZil04S6"
      },
      "source": [
        "## Google Drive\n",
        "\n",
        "Mount _Google Drive_ for SAR images (recomended to store input and output images).\n",
        "\n",
        "> If _Google Colab_ is not used, then this cell may be commented out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHBd4tRMJZru"
      },
      "source": [
        "from os import path as osp\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "PATH_DRIVE = osp.join('/', 'content', 'drive')\n",
        "\n",
        "# Do not mount if it is already attached\n",
        "if not osp.exists(PATH_DRIVE):\n",
        "    print(\"Mounting Google Drive...\")\n",
        "    drive.mount(PATH_DRIVE)\n",
        "else:\n",
        "    print(\"Google Drive has been already mounted!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSdXi6F2aWMs"
      },
      "source": [
        "## Paths\n",
        "\n",
        "Paths to be used by preprocessing steps. Use `PATH_STORAGE` as a subdirectory hierarchy to store processed GeoTIFFs/Shapefiles right into _Google Drive_ (empty string will make saving to _Google Drive_'s root into folders `input`/`output`). `PATH_STORAGE` is used with the `PATH_DRIVE` variable only.\n",
        "\n",
        "`PATH_TEMP` is used to store intermediate GeoTIFFs while processing.\n",
        "\n",
        "`PATH_INPUT` is used as a source of GeoTIFFs (differs from _dataset_ directory in that it have **no masks**).\n",
        "\n",
        "`PATH_OUTPUT` is used to save images (for example, inference on a test subset).\n",
        "\n",
        "`PATH_MODELS` is used to save model weights for later inference.\n",
        "\n",
        "`PATH_DATASET` is a source of GeoTIFF **images** and **masks** for training and inference (for example, test subset).\n",
        "\n",
        "`PATH_RESOURCES` is used as a source of auxiliary files such as GeoJSON search area or Shapefile cutline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4vFyT2LTzN7"
      },
      "source": [
        "from os import path as osp\n",
        "\n",
        "\n",
        "PATH_STORAGE = osp.join('ods', 'soc')  # arbitrary subpath in Google Drive (if any)\n",
        "if 'PATH_DRIVE' in locals():\n",
        "    PREFIX_DRIVE = osp.join(osp.basename(PATH_DRIVE), 'MyDrive', PATH_STORAGE)\n",
        "else:\n",
        "    PREFIX_DRIVE = ''\n",
        "\n",
        "PATH_TEMP = osp.join('/', 'content', 'temp')\n",
        "PATH_INPUT = osp.join('/', 'content', PREFIX_DRIVE, 'input')\n",
        "PATH_OUTPUT = osp.join('/', 'content', PREFIX_DRIVE, 'output')\n",
        "PATH_MODELS = osp.join('/', 'content', PREFIX_DRIVE, 'models')\n",
        "PATH_DATASET = osp.join('/', 'content', PREFIX_DRIVE, 'dataset')\n",
        "PATH_RESOURCES = osp.join('/', 'content', 'resources')\n",
        "\n",
        "# FILE_SHAPEFILE = osp.join(PATH_RESOURCES, 'clustering', 'cutline',\n",
        "#                           'Start_Ice_Map_UTMz40WGS84f_r.shp')\n",
        "\n",
        "print('\\n'.join((PATH_STORAGE, PATH_TEMP, PATH_INPUT, PATH_OUTPUT, PATH_MODELS,\n",
        "                 PATH_DATASET, PATH_RESOURCES)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vIWdMcLXMxZ"
      },
      "source": [
        "## Functions\n",
        "\n",
        "Auxiliary functions (for example, drawing/plotting data)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-CGHBQFXQ44"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "def draw_one_row(*images, size=1024):\n",
        "    try:\n",
        "        size = size[:2]\n",
        "    except:\n",
        "        size = (size, size)\n",
        "    count = len(images)\n",
        "    figure, axes = plt.subplots(1, count, dpi=72,\n",
        "                                figsize=(size[0] / 72, size[1] / 72))\n",
        "    for i in range(count):\n",
        "        axes[i].imshow(images[i])\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y52v0cNRKAs6"
      },
      "source": [
        "# Data Loading\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaDLWDQsmbyK"
      },
      "source": [
        "## Dataset class\n",
        "\n",
        "The most interesting part of the pipeline, 'cause it's an interface between data and the neural network models. Here is an example of a dataset class (`DatasetSAR`) which loads a pair of HH+HV images and their corresponding masks from different directories, stacks them together, applies goemetrics transformations and color augmentations (here the general _augmentations_ term was split into geometric and color in such a way).\n",
        "\n",
        "Parameters for the `DatasetSAR` class constructor:\n",
        "\n",
        "* `paths_images` is a list of strings (or a single string in case of only one polarization used) where each string point to an existing directory with HH or HV polarized images;\n",
        "* `paths_masks` is a list of strings / single string that point(s) to where masks are located (only first directory from the list is used);\n",
        "* `classes` is a list of classes used — important for multiclass masks (where they should be **expanded** into one-hot encoded tensor);\n",
        "* `items` is a list of filenames to be used as the dataset items (WARNING: passed list of filenames is acceted as-is with no checks) — useful for creating subsets (for example, train/valid) from some superset (list of all items of the dataset);\n",
        "* `transformations` is a [torchvision transforms](https://pytorch.org/vision/stable/transforms.html) class (or any other compatible callable class) **without** things like `ToTensor` (it is added separately to the images only) that is applied to both images and masks;\n",
        "* `augmentations` is a callable like `transformations`, but is applied to images only;\n",
        "* `size` is an integer or 2x tuple of integers for final image/mask output size (for example, 1024x1024 as input image for Unet with EfficientNet-B2 encoder);\n",
        "* `expand` is a boolean flag which true value enables expanding a mask into one-hot encoded tensor (that is required for some loss functions and their modes, default is `False` — do not expand);\n",
        "* `flat` is a boolean flag which true value produces squeezed mask (`(H, W)` instead of `(C, H, W)`, for example, for Focal Loss target) — takes precedence over `expand` flag (if `flat` then it's never `expand`ed).\n",
        "\n",
        "> DEBUG: commented line with `np.clip` is for testing binary segmentation.\n",
        "\n",
        "> TODO: add loading and geometric transformations for RGB images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHVKXKfwKF2d"
      },
      "source": [
        "from collections import OrderedDict\n",
        "from glob import glob\n",
        "\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision.transforms import Compose, Lambda, RandomResizedCrop, Resize, ToTensor\n",
        "from torchvision.transforms import InterpolationMode\n",
        "\n",
        "\n",
        "class DatasetSAR(Dataset):\n",
        "    def __init__(self, paths_images, paths_masks, classes, items=None,\n",
        "                 transformations=None, augmentations=None, size=None,\n",
        "                 expand=False, flat=False):\n",
        "        # Assert images paths\n",
        "        if isinstance(paths_images, str):\n",
        "            paths_images = (paths_images, path_images)\n",
        "        elif not isinstance(paths_images, (tuple, list, set)):\n",
        "            raise TypeError(\"first argument must be of type str or list!\")\n",
        "        else:\n",
        "            paths_images = tuple(paths_images)\n",
        "        # Assert masks paths\n",
        "        if isinstance(paths_masks, str):\n",
        "            paths_masks = (paths_masks,)\n",
        "        elif not isinstance(paths_masks, (tuple, list, set)):\n",
        "            raise TypeError(\"second argument must be of type str or list!\")\n",
        "        else:\n",
        "            paths_masks = tuple(paths_masks)\n",
        "        # Default output image/mask size\n",
        "        if size is None:\n",
        "            size = (1024, 1024)\n",
        "        # Default transformations (geometric - image + mask)\n",
        "        mode_interpolation = InterpolationMode.NEAREST\n",
        "        if not isinstance(transformations, (Compose, torch.nn.Module)):\n",
        "            self.transformations = Compose([\n",
        "                Resize(size, mode_interpolation),\n",
        "            ])\n",
        "        else:\n",
        "            self.transformations = transformations\n",
        "        # Default augmentations (color - image only)\n",
        "        if not isinstance(augmentations, (Compose, torch.nn.Module)):\n",
        "            self.augmentations = Compose([\n",
        "                Lambda(lambda x: x),\n",
        "            ])\n",
        "        else:\n",
        "            self.augmentations = augmentations\n",
        "        # Class attributes\n",
        "        self.to_tensor = ToTensor()\n",
        "        self.paths_images = paths_images\n",
        "        self.paths_masks = paths_masks\n",
        "        if items is None:\n",
        "            items = []\n",
        "            for path in paths_images[:2] + paths_masks[:1]:\n",
        "                items.append({osp.basename(item) for item \\\n",
        "                            in glob(osp.join(path, '*.tiff'))})\n",
        "            self.items = sorted(items[0].intersection(*items))\n",
        "        else:\n",
        "            self.items = items\n",
        "        self.classes = len(classes)\n",
        "        self.items_class = OrderedDict({c: i for c, i \\\n",
        "                                        in zip(classes,\n",
        "                                               range(1, self.classes + 1))})\n",
        "        self.expand = expand\n",
        "        self.flat = flat\n",
        "        return None\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        # TODO: load RGBs\n",
        "        image_hh = cv.imread(osp.join(self.paths_images[0], self.items[item]),\n",
        "                             cv.IMREAD_LOAD_GDAL | cv.IMREAD_GRAYSCALE)\n",
        "        h, w = image_hh.shape[:2]\n",
        "        image_hv = cv.imread(osp.join(self.paths_images[1], self.items[item]),\n",
        "                             cv.IMREAD_LOAD_GDAL | cv.IMREAD_GRAYSCALE)\n",
        "        h, w = tuple(map(min, zip(image_hv.shape[:2], (h, w))))\n",
        "        if self.paths_masks:\n",
        "            mask = cv.imread(osp.join(self.paths_masks[0], self.items[item]),\n",
        "                            cv.IMREAD_LOAD_GDAL | cv.IMREAD_GRAYSCALE)\n",
        "        else:\n",
        "            mask = image_hh\n",
        "        h, w = tuple(map(min, zip(mask.shape[:2], (h, w))))\n",
        "        # Use minimum height and width 'cause image/mask dimension may not\n",
        "        # always fit (difference during mask conversion)\n",
        "        image = Image.fromarray(np.dstack(\n",
        "            [image_hh[:h, :w], image_hv[:h, :w], mask[:h, :w]]\n",
        "            # [image_hh[:h, :w], image_hv[:h, :w], np.clip(mask[:h, :w], 0, 1)]\n",
        "        ))\n",
        "        del image_hh, image_hv, mask\n",
        "        image = self.transformations(image)\n",
        "        image = np.array(image)\n",
        "        image, mask = image[..., :2], image[..., 2]\n",
        "        image = self.to_tensor(np.dstack((image[..., 1], image)))\n",
        "        image = self.augmentations(image)\n",
        "        # Convert to int64 'cause OHE requires index tensor\n",
        "        if self.flat:\n",
        "            mask = torch.tensor(mask)\n",
        "        elif self.expand:\n",
        "            mask = torch.nn.functional.one_hot(torch.tensor(mask,\n",
        "                                                            dtype=torch.int64),\n",
        "                                               self.classes).to(torch.int8)\\\n",
        "                                               .permute(2, 0, 1)\n",
        "        else:\n",
        "            mask = torch.tensor(mask).unsqueeze_(0).to(torch.int64)\n",
        "        return image, mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.items)\n",
        "\n",
        "    def debug(self):\n",
        "        # Debug function to show some dataset stats\n",
        "        print(f\"DEBUG: paths images = {self.paths_images}\")\n",
        "        print(f\"DEBUG: paths masks = {self.paths_masks}\")\n",
        "        print(f\"DEBUG: items ({len(self.items)}):\")\n",
        "        print('\\n'.join(self.items))\n",
        "        print(f\"DEBUG: classes ({self.classes}):\")\n",
        "        print(self.items_class)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fHWXrItk1CS"
      },
      "source": [
        "## Datasets and visualization\n",
        "\n",
        "This part produces datasets to be used in data loaders for training and validation. Here are three datasets: `dataset` — grand superset with all annotated images; `dataset_train` — training subset of grand dataset to be used during training (calculating gradients); `dataset_valid` — validation subset of grand dataset to be used during validation (calculating metrics).\n",
        "\n",
        "> Training/validation items (`items_train`/`items_valid`) are selected evenly across the grand dataset items (feel free to split it in any other way).\n",
        "\n",
        "Transformations are also in two variants (train/valid): **random crop and resize** for training in order to learn features from different scales and simple **resize** for validation (for simplicity and closer approach to later inference scenarios).\n",
        "\n",
        "> All transformations suppose output shape of images/masks to be 1024x1024 (set by `size` valiable).\n",
        "\n",
        "Images and masks from train/valid dataset classes are visualized in the last two loops. As for _MaritimeAI Sentinel-1 Dataset 1920_, there will be displayed 124 images and corresponding masks (neither too much nor too little).\n",
        "\n",
        "Variables `EXPAND` and `FLAT` are used for top-level control of mask output format. For example, for Dice loss from _Segmentation Models_ _utils_ module set `EXPAND=True` and `FLAT=False`, for Dice loss from _Segmentation Models_ _losses_ module set all two variables `False`, for Focal loss from _losses_ module set `FLAT=True`.\n",
        "\n",
        "Variable `DEBUG` sets debug mode for overfitting on one image.\n",
        "\n",
        "Class list used for two-class segmentation (from _MaritimeAI Sentinel-1 Dataset 1920_ `2-class` directory) includes: `nodata` (zero values), `water` (ones) and `ice` (twos).\n",
        "\n",
        "> Formally, there are only `water` and `ice` classes of interest, but technically, all three classes must be specified.\n",
        "\n",
        "> DEBUG: for true binary segmentation there should be one class (for example, `data`).\n",
        "\n",
        "> Yet another variable `NUM_SAMPLES` had been added to control the number of output samples from training and validation datasets (preview data loading transformations and augmentations)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXZSfhywqlGY"
      },
      "source": [
        "from random import sample\n",
        "\n",
        "\n",
        "CLASSES = ['nodata', 'water', 'ice']\n",
        "# CLASSES = ['data']  # do np.clip(mask, 0, 1) for binary mode\n",
        "NUM_SAMPLES = 6  # draw samples from train/valid datasets\n",
        "\n",
        "DEBUG = False\n",
        "EXPAND = True\n",
        "FLAT = True\n",
        "\n",
        "# Nearest interpolation mode is mandatory for masks\n",
        "# and optional (but recommended) for images\n",
        "mode_interpolation = InterpolationMode.NEAREST\n",
        "\n",
        "# Separate transformations for train and valid\n",
        "size = (1024, 1024)  # size for network input\n",
        "transformations_train = Compose([\n",
        "    RandomResizedCrop(size, (0.25, 0.95), (3 / 4, 4 / 3),\n",
        "                      mode_interpolation),\n",
        "])\n",
        "\n",
        "transformations_valid = Compose([\n",
        "    Resize(size, mode_interpolation),\n",
        "])\n",
        "\n",
        "# Images/masks paths\n",
        "path_images_hh = osp.join(PATH_DATASET, 'images', 'hh')\n",
        "path_images_hv = osp.join(PATH_DATASET, 'images', 'hv')\n",
        "path_masks = osp.join(PATH_DATASET, 'masks', '2-class')\n",
        "\n",
        "# Grand dataset\n",
        "dataset = DatasetSAR((path_images_hh, path_images_hv), path_masks,\n",
        "                     classes=CLASSES)\n",
        "\n",
        "# Split the dataset into train/valid datasets\n",
        "items_total = len(dataset)\n",
        "fraction = 100 / 15\n",
        "\n",
        "items_train = tuple(dataset.items[i] for i in range(items_total) \\\n",
        "                    if not i or i % int(round(fraction)))\n",
        "\n",
        "items_valid = tuple(dataset.items[i] for i in range(items_total) \\\n",
        "                    if i and not i % int(round(fraction)))\n",
        "\n",
        "# Debug: set default transformations for training and validation\n",
        "if DEBUG:\n",
        "    transformations_train = None\n",
        "    transformations_valid = None\n",
        "\n",
        "dataset_train = DatasetSAR((path_images_hh, path_images_hv), path_masks,\n",
        "                           items=items_train, classes=CLASSES, expand=EXPAND,\n",
        "                           flat=FLAT, transformations=transformations_train)\n",
        "\n",
        "dataset_valid = DatasetSAR((path_images_hh, path_images_hv), path_masks,\n",
        "                           items=items_valid, classes=CLASSES, expand=EXPAND,\n",
        "                           flat=FLAT, transformations=transformations_valid)\n",
        "\n",
        "# Debug: make datasets of one item\n",
        "if DEBUG:\n",
        "    dataset_train.items = dataset.items[:1]\n",
        "    dataset_valid.items = dataset.items[:1]\n",
        "\n",
        "nomasks = []\n",
        "size_train = len(dataset_train)\n",
        "print(f\"Training dataset ({size_train}):\")\n",
        "for i in sample(range(size_train), k=min(NUM_SAMPLES, size_train)):\n",
        "    try:\n",
        "        image, mask = dataset_train[i]\n",
        "        draw_one_row(image.permute(1, 2, 0), mask if FLAT else mask.argmax(0) \\\n",
        "                     if EXPAND else mask.squeeze(0) if not FLAT else mask)\n",
        "    except AttributeError:\n",
        "        nomask = dataset_train.items[i]\n",
        "        nomasks.append(nomask)\n",
        "        print(f\"ERROR: failed to load {nomask}\")\n",
        "print(f\"Read errors = {len(nomasks)}\")\n",
        "\n",
        "nomasks = []\n",
        "size_valid = len(dataset_valid)\n",
        "print(f\"Validation dataset ({size_valid}):\")\n",
        "for i in sample(range(size_valid), k=min(NUM_SAMPLES, size_valid)):\n",
        "    try:\n",
        "        image, mask = dataset_valid[i]\n",
        "        draw_one_row(image.permute(1, 2, 0), mask if FLAT else mask.argmax(0) \\\n",
        "                     if EXPAND else mask.squeeze(0))\n",
        "    except AttributeError:\n",
        "        nomask = dataset_valid.items[i]\n",
        "        nomasks.append(nomask)\n",
        "        print(f\"ERROR: failed to load {nomask}\")\n",
        "print(f\"Read errors = {len(nomasks)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQvSiMDNy7zJ"
      },
      "source": [
        "`DatasetSAR` class has optional `debug` method to show filenames of items and some other info."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWXaIVDeraup"
      },
      "source": [
        "# dataset.debug()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75EX9_pIqPo8"
      },
      "source": [
        "## Dataloaders\n",
        "\n",
        "_Google Colab_ settings. Batch size can be increased, also number of workers, if neccessary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLyc1dHhmHZO"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "loader_train = DataLoader(dataset_train, batch_size=1, shuffle=True,\n",
        "                          num_workers=2)\n",
        "\n",
        "loader_valid = DataLoader(dataset_valid, batch_size=1, shuffle=False,\n",
        "                          num_workers=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wTOlKWdKDc1"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYu0FtN6mRzj"
      },
      "source": [
        "## Model\n",
        "\n",
        "**Unet** with **EfficientNet-B2** encoder is used as an example 'cause it does fit into _Google Colab_ GPU RAM. EfficientNet-B3 may also fit (larger encoders like EfficientNet-B7 were tested on local devboxes and gave the best results).\n",
        "\n",
        "For some losses from _Segmentation Models_ it may require to change mask output format (squeezed or one-hot expanded, see `EXPAND` and `FLAT` variables for datasets).\n",
        "\n",
        "Experimenting with **loss functions** may give interesting results. Compare the same scenario with different loss functions:\n",
        "\n",
        "* DiceLoss (with classes 1 and 2)\n",
        "\n",
        "![DiceLoss](https://raw.githubusercontent.com/MaritimeAI/ODS-SoC/1b066dd63aedd364fda4ad0d08ac3ae6286f2289/assets/images/dice_12.png)\n",
        "\n",
        "* FocalLoss\n",
        "\n",
        "![FocalLoss](https://raw.githubusercontent.com/MaritimeAI/ODS-SoC/1b066dd63aedd364fda4ad0d08ac3ae6286f2289/assets/images/focal_all.png)\n",
        "\n",
        "This is a very basic example, so classic **metrics** such as **IoU** and **optimizer** such as **Adam** are being used.\n",
        "\n",
        "Training and validation **runners** are killer feature of _Segmentation Models_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9Fz8dW3FfuY"
      },
      "source": [
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "\n",
        "device = ['cpu', 'cuda'][torch.cuda.is_available()]\n",
        "\n",
        "# Model\n",
        "model = smp.Unet(encoder_name='efficientnet-b2', encoder_weights='imagenet',\n",
        "                 in_channels=3, classes=len(CLASSES))\n",
        "\n",
        "# Loss functions\n",
        "# loss = smp.utils.losses.DiceLoss()\n",
        "mode = smp.losses.MULTICLASS_MODE\n",
        "# loss = smp.losses.DiceLoss(mode=mode, classes=[1, 2])\n",
        "# setattr(loss, '__name__', 'dice_loss')\n",
        "loss = smp.losses.FocalLoss(mode=mode)\n",
        "setattr(loss, '__name__', 'focal_loss')\n",
        "\n",
        "# Metrics\n",
        "metrics = [\n",
        "           smp.utils.metrics.IoU(threshold=0.5)\n",
        "]\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.Adam([{\n",
        "    'params': model.parameters(),\n",
        "    'lr': 2e-4\n",
        "}])\n",
        "\n",
        "# Runners\n",
        "train_one_epoch = smp.utils.train.TrainEpoch(model, loss=loss, metrics=metrics,\n",
        "                                             optimizer=optimizer, device=device,\n",
        "                                             verbose=True)\n",
        "valid_one_epoch = smp.utils.train.ValidEpoch(model, loss=loss, metrics=metrics,\n",
        "                                             device=device, verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFhh_g8fmV55"
      },
      "source": [
        "## Train loop\n",
        "\n",
        "All the models are saved to `PATH_MODELS` under a separate subdirectory with timestamp in its name.\n",
        "\n",
        "The best score and the **best** corresponding **weights** are saved after `EPOCHS_MIN` (10th by default) epoch. Final **last weights** are saved at the end (if everything goes well or after `EPOCHS_MIN` epochs if an error occurs).\n",
        "\n",
        "**Learning rate** is decreased by 5% every epoch.\n",
        "\n",
        "> `TRAIN=False` allows to skip training and just load a saved model. In order to start training from a scratch, just set `TRAIN=True` and `NAME_PRELOAD=''` (both variables must not evaluate to `False`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JNtzQ4oqyYi"
      },
      "source": [
        "import os\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "NAME_PRELOAD = ''\n",
        "FORMAT_DATE = '%Y-%m-%d-%H-%M-%S'\n",
        "EPOCHS_MIN = 10\n",
        "EPOCHS = 50\n",
        "TRAIN = True\n",
        "\n",
        "timestamp = datetime.utcnow().strftime(FORMAT_DATE)\n",
        "path_model_best = osp.join(PATH_MODELS, timestamp, 'best.pth')\n",
        "path_model_last = osp.join(PATH_MODELS, timestamp, 'last.pth')\n",
        "path_model_onnx = osp.join(PATH_MODELS, timestamp, 'model.onnx')\n",
        "score_max = 0\n",
        "\n",
        "if NAME_PRELOAD:\n",
        "    try:\n",
        "        path_model_resume = path_model_last.replace(timestamp, NAME_PRELOAD)\n",
        "        model = torch.load(path_model_resume, map_location=device)\n",
        "        print(f\"The model has been successfully loaded from {NAME_PRELOAD}!\")\n",
        "    finally:\n",
        "        pass\n",
        "\n",
        "if TRAIN:\n",
        "    os.makedirs(osp.dirname(path_model_best), exist_ok=True)\n",
        "    os.makedirs(osp.dirname(path_model_last), exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        for i in range(EPOCHS):\n",
        "            print(f\"Epoch = {i:3d}, learning rate =\",\n",
        "                f\"{optimizer.param_groups[0]['lr']:0.8f}\")\n",
        "\n",
        "            logs_train = train_one_epoch.run(loader_train)\n",
        "            logs_valid = valid_one_epoch.run(loader_valid)\n",
        "\n",
        "            if score_max < logs_valid['iou_score'] and i >= EPOCHS_MIN:\n",
        "                # Save only after 10 epochs\n",
        "                score_max = logs_valid['iou_score']\n",
        "                torch.save(model, path_model_best)\n",
        "                print(f\"Saved best model with score = {score_max:0.4f}!\")\n",
        "\n",
        "            # Unconditional model saving\n",
        "            torch.save(model, path_model_last)\n",
        "            print(f\"Saved latest model with score =\",\n",
        "                  f\"{logs_valid['iou_score']:0.4f}!\")\n",
        "\n",
        "            optimizer.param_groups[0]['lr'] *= 0.95  # step down\n",
        "            print()\n",
        "    except KeyboardInterrupt:\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDJ48Wv7hj-P"
      },
      "source": [
        "## Export to ONNX\n",
        "\n",
        "Exporting a model to _ONNX_ format enables it to be run on any platform optimized for CPU or/and GPU.\n",
        "\n",
        "> Exporting a model to _ONNX_ format with _PyTorch_ does not require _ONNX_ dependencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWQZjS4xid0J"
      },
      "source": [
        "if 'model' in locals() and hasattr(model, 'predict'):\n",
        "    os.makedirs(osp.dirname(path_model_onnx), exist_ok=True)\n",
        "    model = model.cpu()  # inference on CPU\n",
        "\n",
        "    # Images to use during export to ONNX\n",
        "    images, masks_true = [], []\n",
        "    for image, mask_true in loader_valid:\n",
        "        images.append(image)\n",
        "        masks_true.append(mask_true)\n",
        "        break\n",
        "\n",
        "    # PyTorch prediction for later comparison with ONNX model\n",
        "    masks_predict = []\n",
        "    for image in images:\n",
        "        # masks_predict.append(model.predict(image.to(device)).cpu())\n",
        "        masks_predict.append(model.predict(image.cpu()))\n",
        "\n",
        "    # Make EfficientNet TorchScript-friendly\n",
        "    model.encoder.set_swish(memory_efficient=False)\n",
        "\n",
        "    # Script and export model to ONNX\n",
        "    # without dynamic_axes argument resulting model will have\n",
        "    # the same dimension size as input during export\n",
        "    torch.onnx.export(model, tuple(image.to(device) for image in images),\n",
        "                    path_model_onnx, export_params=True,\n",
        "                    opset_version=11, #do_constant_folding=True,\n",
        "                    input_names=['input'], output_names=['output'],\n",
        "                    dynamic_axes={\n",
        "                        #   'input': {0: 'batch_size'},\n",
        "                        #   'output': {0, 'batch_size'}\n",
        "                    }, # operator_export_type=torch.onnx.OperatorExportTypes.ONNX_ATEN_FALLBACK\n",
        "                    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpLvUN2wBWKk"
      },
      "source": [
        "It may be ok to clean memory after each training (if the same model is not going to be trained twice)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Htynb7T-AcMo"
      },
      "source": [
        "try:\n",
        "    del model\n",
        "except:\n",
        "    pass\n",
        "\n",
        "try:\n",
        "    torch.cuda.empty_cache()\n",
        "except:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6GRMAQ6tp3F"
      },
      "source": [
        "# Inference\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wS6WV4q4mCGd"
      },
      "source": [
        "## Validation subset\n",
        "\n",
        "Inference on validation subset makes sense in combination with visualization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrqpKOrqmFvm"
      },
      "source": [
        "### Inference and visualization (PyTorch)\n",
        "\n",
        "Use last or best model. After each inference memory is going to be cleaned."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "974_s5_Mcaer"
      },
      "source": [
        "VALID = True\n",
        "\n",
        "if VALID and osp.exists(path_model_last):\n",
        "    model_eval = torch.load(path_model_last)\n",
        "\n",
        "    for image, mask_true in loader_valid:\n",
        "        mask_predict = model_eval.predict(image.to(device)).cpu()\n",
        "        for i, p, t in zip(image, mask_predict, mask_true):\n",
        "            draw_one_row(i.permute(1, 2, 0), p.argmax(0), t.squeeze(0))\n",
        "\n",
        "try:\n",
        "    del model_eval\n",
        "except:\n",
        "    pass\n",
        "\n",
        "try:\n",
        "    torch.cuda.empty_cache()\n",
        "except:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdoy9oBglnIV"
      },
      "source": [
        "## Test subset (using ONNX runtime)\n",
        "\n",
        "Test subset for images without annotations. So noone knows the ground truth masks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxkGI4PLnl1A"
      },
      "source": [
        "### Prepare ONNX\n",
        "\n",
        "Install from _PyPI_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkwDZWt_KBIE"
      },
      "source": [
        "%pip install onnx onnxruntime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_xWFCAYNNBc"
      },
      "source": [
        "Check the model with _ONNX checker_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWW4xzp-nk5K"
      },
      "source": [
        "import onnx\n",
        "\n",
        "if osp.exists(path_model_onnx):\n",
        "    model_onnx = onnx.load(path_model_onnx)\n",
        "    onnx.checker.check_model(model_onnx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBJZWUQGx0B1"
      },
      "source": [
        "Check the model with _ONNX Runtime_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGyu_EuEK0HL"
      },
      "source": [
        "import onnxruntime\n",
        "\n",
        "def to_numpy(tensor):\n",
        "    return tensor.detach().cpu().numpy() if tensor.requires_grad \\\n",
        "           else tensor.cpu().numpy()\n",
        "\n",
        "if osp.exists(path_model_onnx):\n",
        "    session_ort = onnxruntime.InferenceSession(path_model_onnx)\n",
        "\n",
        "    inputs_ort = {session_ort.get_inputs()[0].name: to_numpy(images[0])}\n",
        "    outputs_ort = session_ort.run(None, inputs_ort)\n",
        "\n",
        "    np.testing.assert_allclose(to_numpy(masks_predict[0]), outputs_ort[0],\n",
        "                            rtol=1e-3, atol=9e-2)\n",
        "\n",
        "    print(\"OK: ONNX model verification complete!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLVgtxsqxHHn"
      },
      "source": [
        "Draw the inference result for the exported model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSSXsyxsN4e6"
      },
      "source": [
        "if (len(set(('images', 'masks_predict', 'outputs_ort'))\\\n",
        "        .intersection(set(locals())))) == 3:\n",
        "    draw_one_row(images[0][0].permute(1, 2, 0),\n",
        "                masks_predict[0][0].argmax(0),\n",
        "                outputs_ort[0][0].argmax(0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AS2tWxTLC77w"
      },
      "source": [
        "### Data loading\n",
        "\n",
        "Reuse variables `path_images_hh`, `path_images_hv` and `path_masks` from previous cells."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8fSdWlNpmyy"
      },
      "source": [
        "items_images = set(os.listdir(path_images_hh))\n",
        "items_masks = set(os.listdir(path_masks))\n",
        "items_test = sorted(items_images - items_masks)\n",
        "dict(enumerate(items_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfJDZ1GpleIX"
      },
      "source": [
        "### Inference and visualization (ONNX)\n",
        "\n",
        "Use vectorized cutline from repository to crop `NoData` area."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtwMnqkE-53z"
      },
      "source": [
        "!git clone https://github.com/MaritimeAI/resources.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HBNUh3AtXH4"
      },
      "source": [
        "> Cutline shape may also be checked with `ogrinfo` utility from _GDAL_ library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jmhm3t1197dx"
      },
      "source": [
        "FILE_SHAPEFILE = osp.join(PATH_RESOURCES, 'clustering', 'cutline',\n",
        "                          'Start_Ice_Map_UTMz40WGS84f_r.shp')\n",
        "\n",
        "try:\n",
        "    if osp.isfile(FILE_SHAPEFILE):\n",
        "        shape = osp.abspath(osp.realpath(FILE_SHAPEFILE))\n",
        "    else:\n",
        "        raise FileNotFoundError\n",
        "except (TypeError, FileNotFoundError) as e:\n",
        "    print(f\"Shapefile '{FILE_SHAPEFILE}' does not exist!\")\n",
        "    shape = None\n",
        "print(f\"Available shape is {shape}\")\n",
        "# !ogrinfo \"{shape}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywwf5u83_Z8x"
      },
      "source": [
        "Inference part itself that does not depend on _PyTorch_ or any model code — just _ONNX_ exported model and _ONNX Runtime_.\n",
        "\n",
        "> HH + HV polarizations are being combined during inference, just like in `DatasetSAR` class.\n",
        "\n",
        "> _Python_'s garbage collector is being used intensively here, 'cause images are really large."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nmm5Dv1DeXuH"
      },
      "source": [
        "import gc\n",
        "\n",
        "import onnxruntime\n",
        "\n",
        "from tempfile import TemporaryDirectory\n",
        "\n",
        "from osgeo import gdal\n",
        "\n",
        "PLOT = True\n",
        "\n",
        "# Target directory for test inference\n",
        "path_test = osp.join(PATH_OUTPUT, timestamp, 'test')\n",
        "os.makedirs(path_test, exist_ok=True)\n",
        "\n",
        "# Batch size is supposed to be 1\n",
        "for i, item_test in enumerate(items_test):\n",
        "    source = osp.join(path_images_hh, item_test)\n",
        "    target = osp.join(path_test, item_test)\n",
        "    with TemporaryDirectory() as path_temp:\n",
        "        temp = osp.join(path_temp, item_test)\n",
        "        gdal.Translate(temp, source,\n",
        "                    options=['-b', '1', '-colorinterp', 'gray',\n",
        "                                '-co', 'COMPRESS=DEFLATE'])\n",
        "\n",
        "        # Input images must be 8-bit GeoTIFFs\n",
        "        image_hh = cv.imread(osp.join(path_images_hh, item_test), cv.IMREAD_LOAD_GDAL)\n",
        "        image_hv = cv.imread(osp.join(path_images_hv, item_test), cv.IMREAD_LOAD_GDAL)\n",
        "\n",
        "        # Make HH and HV sizes match (sizes mismatch should never happen)\n",
        "        # image_hv = cv.resize(image_hv, image_hh.shape[::-1], cv.INTER_NEAREST)\n",
        "        image = np.dstack((image_hv, image_hh, image_hv)) / np.float32(255)\n",
        "        del image_hh, image_hv\n",
        "        gc.collect()\n",
        "\n",
        "        image = cv.resize(image, (1024, 1024), cv.INTER_LINEAR)\n",
        "        # Output image (GeoTIFF copy of image_hh)\n",
        "        dataset = gdal.Open(temp, gdal.GA_Update)\n",
        "        band = dataset.GetRasterBand(1)\n",
        "        h, w = band.ReadAsArray().shape\n",
        "        inputs_ort = {\n",
        "            session_ort.get_inputs()[0].name: (np.moveaxis(image, -1, 0)[None, ...])\n",
        "        }\n",
        "        outputs_ort = session_ort.run(None, inputs_ort)\n",
        "        # WARNING: this resize part works just because there are 3 classes\n",
        "        mask = cv.resize(np.moveaxis(outputs_ort[0][0], 0, -1),\n",
        "                        (w, h), cv.INTER_NEAREST)\n",
        "        mask[..., 2] *= 2  # change 'ice' class weight\n",
        "        mask = mask.argmax(-1).clip(0, 255).astype('uint8')\n",
        "        band.WriteArray(mask)\n",
        "        dataset.FlushCache()\n",
        "        del band, dataset\n",
        "        gc.collect()\n",
        "\n",
        "        # Assume NoData value is always zero\n",
        "        gdal.Warp(target, temp, dstNodata=0, xRes=40, yRes=40,\n",
        "                  cutlineDSName=f\"{shape}\",\n",
        "                  cropToCutline=(True if shape else False),\n",
        "                  creationOptions=['COMPRESS=DEFLATE'])\n",
        "        if PLOT:\n",
        "            mask = cv.imread(target, cv.IMREAD_LOAD_GDAL)\n",
        "            draw_one_row(cv.resize(image, (w, h), cv.INTER_LINEAR), mask)\n",
        "        del image, mask\n",
        "    gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGfbJCpdB1iW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}