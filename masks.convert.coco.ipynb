{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "convert.masks.coco.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae1xK-yJXn7M"
      },
      "source": [
        "# Prerequisites\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCKrWAfIdzQs"
      },
      "source": [
        "\n",
        "## Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgcvZcU-fcWw"
      },
      "source": [
        "# Dark magic happens here: installing dependencies for GDAL 3.0.4\n",
        "# build process via APT and install GDAL itself via PyPI\n",
        "!time (add-apt-repository -y ppa:ubuntugis/ubuntugis-unstable && \\\n",
        " apt install python3-gdal=3.0.4+dfsg-1~bionic0 && \\\n",
        " apt purge --autoremove python3-gdal && \\\n",
        " pip install gdal==3.0.4 && \\\n",
        " apt install gdal-bin=3.0.4+dfsg-1~bionic0)\n",
        "\n",
        "from osgeo import gdal; print(f\"GDAL version {(gdal.__version__)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqBkfW1bXvX-"
      },
      "source": [
        "## Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cml7H-vCgB2d"
      },
      "source": [
        "from os import path as osp\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "PATH_DRIVE = osp.join('/', 'content', 'drive')\n",
        "\n",
        "# Do not mount if it is already attached\n",
        "if not osp.exists(PATH_DRIVE):\n",
        "    print(\"Mounting Google Drive...\")\n",
        "    drive.mount(PATH_DRIVE)\n",
        "else:\n",
        "    print(\"Google Drive has been already mounted!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-OzliSNXyvk"
      },
      "source": [
        "## Paths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJmF1-FVgBzi"
      },
      "source": [
        "from os import path as osp\n",
        "\n",
        "\n",
        "PATH_STORAGE = osp.join('ods', 'soc')  # arbitrary subpath in Google Drive (if any)\n",
        "if 'PATH_DRIVE' in locals():\n",
        "    PREFIX_DRIVE = osp.join(osp.basename(PATH_DRIVE), 'MyDrive', PATH_STORAGE)\n",
        "else:\n",
        "    PREFIX_DRIVE = ''\n",
        "\n",
        "PATH_TEMP = osp.join('/', 'content', 'temp')\n",
        "PATH_INPUT = osp.join('/', 'content', PREFIX_DRIVE, 'input')\n",
        "PATH_OUTPUT = osp.join('/', 'content', PREFIX_DRIVE, 'output')\n",
        "PATH_MODELS = osp.join('/', 'content', PREFIX_DRIVE, 'models')\n",
        "PATH_DATASET = osp.join('/', 'content', PREFIX_DRIVE, 'dataset')\n",
        "PATH_RESOURCES = osp.join('/', 'content', 'resources')\n",
        "\n",
        "# FILE_SHAPEFILE = osp.join(PATH_RESOURCES, 'clustering', 'cutline',\n",
        "#                           'Start_Ice_Map_UTMz40WGS84f_r.shp')\n",
        "\n",
        "print('\\n'.join((PATH_STORAGE, PATH_TEMP, PATH_INPUT, PATH_OUTPUT, PATH_MODELS,\n",
        "                 PATH_DATASET, PATH_RESOURCES)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bS2EEJTX18K"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "Exclude bad images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OERaxeILgBvF"
      },
      "source": [
        "items_exclude_a = [\n",
        "    'S1B_EW_GRDM_1SDH_20200203T031613_20200203T031631_020099_0260A6_D03B.tiff',\n",
        "    'S1B_EW_GRDM_1SDH_20200215T031613_20200215T031630_020274_026647_9E25.tiff',\n",
        "    'S1B_EW_GRDM_1SDH_20200227T031613_20200227T031630_020449_026BE9_3282.tiff',\n",
        "    'S1B_EW_GRDM_1SDH_20200310T031613_20200310T031630_020624_027178_1A36.tiff',\n",
        "    'S1B_EW_GRDM_1SDH_20200322T031613_20200322T031631_020799_027702_664C.tiff',\n",
        "    'S1B_EW_GRDM_1SDH_20200521T031615_20200521T031633_021674_029249_923C.tiff',\n",
        "]\n",
        "\n",
        "items_exclude_b = [\n",
        "    'S1A_EW_GRDM_1SDH_20191117T031700_20191117T031800_029945_036ADD_32F2.tiff',\n",
        "    'S1A_EW_GRDM_1SDH_20191129T031659_20191129T031759_030120_0370EF_D07E.tiff',\n",
        "    'S1A_EW_GRDM_1SDH_20200104T031658_20200104T031758_030645_038306_DDA1.tiff',\n",
        "    'S1A_EW_GRDM_1SDH_20200328T031656_20200328T031756_031870_03ADB5_D992.tiff',\n",
        "    'S1A_EW_GRDM_1SDH_20200421T031657_20200421T031757_032220_03BA08_1F43.tiff',\n",
        "]\n",
        "\n",
        "items_exclude_c = [\n",
        "    'S1A_EW_GRDM_1SDH_20191211T031659_20191211T031759_030295_0376F4_BE3E.tiff',\n",
        "    'S1A_EW_GRDM_1SDH_20191223T031658_20191223T031758_030470_037CFD_AB38.tiff',\n",
        "    'S1A_EW_GRDM_1SDH_20200221T031656_20200221T031756_031345_039B6D_927B.tiff',\n",
        "    'S1A_EW_GRDM_1SDH_20200304T031656_20200304T031756_031520_03A17D_08EB.tiff',\n",
        "    'S1A_EW_GRDM_1SDH_20200316T031656_20200316T031756_031695_03A78C_D3A3.tiff',\n",
        "    'S1A_EW_GRDM_1SDH_20200409T031657_20200409T031757_032045_03B3E6_7A01.tiff',\n",
        "    'S1A_EW_GRDM_1SDH_20200503T031658_20200503T031758_032395_03C031_950B.tiff',\n",
        "]\n",
        "\n",
        "items_exclude_d = [\n",
        "    'S1A_EW_GRDM_1SDH_20191107T030034_20191107T030132_029799_0365BB_F7CF.tiff',\n",
        "    'S1B_EW_GRDM_1SDH_20200601T023525_20200601T023601_021834_02971A_B08C.tiff'\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZaqdel2Yqdx"
      },
      "source": [
        "Make splits from filtered data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_t4W6fZDgBsj"
      },
      "source": [
        "import os\n",
        "\n",
        "\n",
        "items = os.listdir(osp.join(PATH_DATASET, 'masks', '2-class'))\n",
        "\n",
        "items_all = [item for item in sorted(items) if item not in \n",
        "                items_exclude_a + items_exclude_b + items_exclude_c +\n",
        "                items_exclude_d]\n",
        "\n",
        "DATA_SPLIT = 0\n",
        "\n",
        "if DATA_SPLIT is None:\n",
        "    items_train = tuple(items_all[i] for i in range(len(items_all)) \\\n",
        "                        if not i or i % int(round(fraction)))\n",
        "\n",
        "    items_valid = tuple(items_all[i] for i in range(len(items_all)) \\\n",
        "                        if i and not i % int(round(fraction)))\n",
        "else:\n",
        "    items_split = [{\n",
        "        'train': sorted(set(items_all) - set(items_all[i::5])),\n",
        "        'valid': sorted(items_all[i::5])\n",
        "    } for i in range(5)]\n",
        "    items_train = items_split[DATA_SPLIT]['train']\n",
        "    items_valid = items_split[DATA_SPLIT]['valid']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dK41-lJyX_cU"
      },
      "source": [
        "Check training / validation ratio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vU5ngaX2kF1i"
      },
      "source": [
        "print(len(items_train), '/', len(items_valid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dn5N9or7Y0Kj"
      },
      "source": [
        "# Annotations\n",
        "\n",
        "At the moment CVAT suffers from inability to convert a task with larger number of classes into a task with smaller number of classes (not all the source classes are uniquely mapped to the target ones). Thus this workaround is being done:\n",
        "\n",
        "* make a copy of the task, so the original (the source, multiclass in our case) task has the same number and order of images as the target (binary in our case) task;\n",
        "* make a dump of the source annotations;\n",
        "* process the source annotations changing it for the target task;\n",
        "* upload the processed annotations to the target task.\n",
        "\n",
        "Several operations are performed further for the target task:\n",
        "\n",
        "1. Multiclass to binary conversion (types of ice into water/ice)\n",
        "2. Replace original polygons with polygons from vectorized masks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4gZ_-YCdwyT"
      },
      "source": [
        "\n",
        "## Path to annotations\n",
        "\n",
        "Select sample annotations dump"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqrtla0BgBnT"
      },
      "source": [
        "name_task_source = 'task_sea_ice_noland_multiclass_30-2021_10_12_16_22_22-coco 1.0.zip'\n",
        "path_annotations = osp.join(PATH_INPUT, 'annotations', name_task_source)\n",
        "\n",
        "print(osp.exists(path_annotations), path_annotations)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSwaABwkWOSZ"
      },
      "source": [
        "## Parse sample annotations dump from CVAT\n",
        "\n",
        "Sample annotations dump is in COCO dataset format for the target task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5uZSmnwgBkn"
      },
      "source": [
        "import json\n",
        "\n",
        "from zipfile import ZipFile\n",
        "\n",
        "with ZipFile(path_annotations, 'r') as archive:\n",
        "    annotations = json.loads(archive.read('annotations/instances_default.json'))\n",
        "\n",
        "annotations.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HxacJv3WGRa"
      },
      "source": [
        "Check original categories (classes)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45I0raIGnLaO"
      },
      "source": [
        "annotations['categories']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UovQLG6NcciP"
      },
      "source": [
        "Copy and rename the source categories (include one extra class for the target)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHHRe4ZxpmfT"
      },
      "source": [
        "categories_binary = annotations['categories'][:3].copy()\n",
        "\n",
        "categories_binary[1]['name'] = 'ice'\n",
        "categories_binary[2]['name'] = 'nodata'\n",
        "\n",
        "categories_binary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrFCC3LCV-Eu"
      },
      "source": [
        "The first and the last COCO image entries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOdfsqASgBIb"
      },
      "source": [
        "annotations['images'][:1] + annotations['images'][-1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uQ9e8z_c841"
      },
      "source": [
        "Check annotation fields taking one of the annotation items"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ODBvxuynGFS"
      },
      "source": [
        "annotations['annotations'][-1].keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlpvaMIldasI"
      },
      "source": [
        "The last annotation item polygon size (points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SCHgDDGrJMO"
      },
      "source": [
        "len(annotations['annotations'][-1]['segmentation'][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQW8Uf1ErX08"
      },
      "source": [
        "## Convert classes\n",
        "\n",
        "Convert category indexes from multiclass source to binary target:\n",
        "\n",
        "| id | source | id | target |\n",
        "| - | - | - | - |\n",
        "| 1 | water | 1 | water |\n",
        "| 2 | nilas | 2 | ice |\n",
        "| 3 | young | 2 | ice |\n",
        "| 4 | first-year | 2 | ice |\n",
        "| 5 | fast | 2 | ice |\n",
        "| - | - | 3 | nodata |\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jVbAlLIpFpH"
      },
      "source": [
        "for annotation in annotations['annotations']:\n",
        "    annotation['category_id'] = min(annotation['category_id'], 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoBCkt7oVpWV"
      },
      "source": [
        "Check unique target categories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYQC0YcKsMkl"
      },
      "source": [
        "set([annotation['category_id'] for annotation in annotations['annotations']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2D_gScR4VhyU"
      },
      "source": [
        "Backup original categories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vIBNuOtsmYW"
      },
      "source": [
        "categories_multiclass = annotations['categories']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVZbMM03e-Vc"
      },
      "source": [
        "Replace categories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDqHBW7vtVGU"
      },
      "source": [
        "annotations['categories'] = categories_binary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZAfBb8he5Nr"
      },
      "source": [
        "Check categories applied"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEZCeDsStiC5"
      },
      "source": [
        "annotations['categories']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDfN-LIXBYhH"
      },
      "source": [
        "import cv2 as cv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from itertools import repeat\n",
        "\n",
        "\n",
        "def get_contours(mask, threshold=1e4, step=1):\n",
        "    # Split values into separate channels (one-hot encoding): HxW -> HxWxC\n",
        "    channels = mask.max() + 1\n",
        "    mask = np.stack([mask == i for i in range(channels)],\n",
        "                    axis=-1).astype('uint8')\n",
        "    list_contours = []\n",
        "    for i in range(channels):\n",
        "        channel = mask[:, :, i]\n",
        "        # Contours is a list of lists of tuples (coordinates)\n",
        "        contours, hierarchy = cv.findContours(channel, cv.RETR_TREE,\n",
        "                                              cv.CHAIN_APPROX_TC89_KCOS)\n",
        "        # Calculate areas for the contours\n",
        "        areas = [round(cv.contourArea(contour)) for contour in contours]\n",
        "        # Reduce points by step parameter (no reduce: step = 1)\n",
        "        contours = [contour[::step, ...].copy() for contour in contours]\n",
        "        # Calculate number of points\n",
        "        points = [len(contour) for contour in contours]\n",
        "\n",
        "        # Append a dataframe for contours, contour areas and number of points\n",
        "        list_contours.append(pd.DataFrame(zip(repeat(i), areas, points, contours),\n",
        "                                          columns=['channel', 'area',\n",
        "                                                   'points', 'contour']))\n",
        "    # Make a dataframe for total contours, contour areas and number of points\n",
        "    df_contours = pd.concat(list_contours)\n",
        "    # Filter out duplicated contours from different channels\n",
        "    count_original = len(df_contours)\n",
        "    df_contours = df_contours.groupby(['area', 'points'], as_index=False).first()\n",
        "    count_filtered = len(df_contours)\n",
        "    # print(f\"DEBUG: filtered out {count_original - count_filtered} duplicated contours!\")\n",
        "    # Filter out very small polygons (here come areas)\n",
        "    count_original = count_filtered\n",
        "    df_contours = df_contours[(df_contours['area'] > threshold) &\n",
        "                              (df_contours['points'] > 2)]\n",
        "    count_filtered = len(df_contours)\n",
        "    # print(f\"DEBUG: filtered out {count_original - count_filtered} small contours!\")\n",
        "    output = [{'channel': channel,\n",
        "               'area': area,\n",
        "               'contour': contour} for contour, area, channel \\\n",
        "              in zip(df_contours['contour'].to_list(),\n",
        "                     df_contours['area'].to_list(),\n",
        "                     df_contours['channel'].to_list())]\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "283OZpZkAjna"
      },
      "source": [
        "annotations_target = annotations.copy()\n",
        "\n",
        "annotations_target['annotations'][0].keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6J6xNF6Bldn"
      },
      "source": [
        "# len([annotation['id'] for annotation in annotations_target['annotations']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQ04Z81ozPNh"
      },
      "source": [
        "annotations_converted = []\n",
        "index_converted = 1\n",
        "map_category = {\n",
        "    0: 3,\n",
        "    1: 1,\n",
        "    2: 2\n",
        "}\n",
        "\n",
        "for n, item in enumerate(items):\n",
        "    path_item = osp.join(PATH_DATASET, 'masks', '2-class', item)\n",
        "    # Vectorize raster mask\n",
        "    image = cv.imread(path_item, cv.IMREAD_LOAD_GDAL)\n",
        "    contours = get_contours(image, threshold=5e3, step=3)\n",
        "    # Find current image id in the target annotations\n",
        "    id_image = [image['id'] for image in annotations_target['images'] \\\n",
        "                if image['file_name'].split('/')[-1] == item][0]\n",
        "    print(f\"{n:3d}: id = {id_image:3d}, item = {item},\",\n",
        "          f\"contours = {len(contours:3d)}\")\n",
        "    # Assemble annotations structure\n",
        "    for contour in contours:\n",
        "        area = contour['area']\n",
        "        points = contour['contour'].squeeze().reshape(-1).tolist()\n",
        "        channel = contour['channel']\n",
        "        # Annotation dictionary\n",
        "        annotation_converted = {}\n",
        "        annotation_converted['area'] = area\n",
        "        annotation_converted['attributes'] = {'occluded': False}\n",
        "        points_horizontal = sorted(points[0::2])\n",
        "        points_vertical = sorted(points[1::2])\n",
        "        annotation_converted['bbox'] = [points_horizontal[0], points_vertical[0],\n",
        "                                        points_horizontal[-1], points_vertical[-1]]\n",
        "        annotation_converted['category_id'] = map_category[channel]\n",
        "        annotation_converted['image_id'] = id_image\n",
        "        annotation_converted['id'] = index_converted\n",
        "        annotation_converted['iscrowd'] = 0\n",
        "        annotation_converted['segmentation'] = [points]\n",
        "        annotations_converted.append(annotation_converted)\n",
        "        index_converted += 1\n",
        "    # break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZgSy-atgCka"
      },
      "source": [
        "Check converted annotations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCLbBbcVO7O_"
      },
      "source": [
        "annotations_converted[0].keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pL98nf8JNGX"
      },
      "source": [
        "len(annotations_converted)#[-1]['segmentation'][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BDQrDt9gH5Z"
      },
      "source": [
        "Replace original annotation with converted ones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8Vd5F77DNJJ"
      },
      "source": [
        "annotations_target['annotations'] = annotations_converted"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwVnBS9XgX9W"
      },
      "source": [
        "Save processed annotations for the target task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_tQS9I0tkKD"
      },
      "source": [
        "from datetime import datetime\n",
        "\n",
        "FORMAT_DATE = '%Y_%m_%d_%H_%M_%S'\n",
        "\n",
        "name_task_target = name_task_source.split('-')\n",
        "name_task_target[-2] = datetime.utcnow().strftime(FORMAT_DATE)\n",
        "name_task_target = '-'.join(name_task_target)\n",
        "print(name_task_target)\n",
        "\n",
        "path_annotations_target = path_annotations.replace(name_task_source,\n",
        "                                                   name_task_target)\n",
        "path_annotations_target = path_annotations_target.replace(PATH_INPUT,\n",
        "                                                          PATH_OUTPUT)\n",
        "print(path_annotations_target)\n",
        "\n",
        "os.makedirs(osp.dirname(path_annotations_target), exist_ok=True)\n",
        "# raise KeyboardInterrupt  # DEBUG\n",
        "\n",
        "with ZipFile(path_annotations_target, 'w') as archive:\n",
        "    archive.writestr('annotations/instances_default.json',\n",
        "                     json.dumps(annotations_target), compresslevel=9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCfewD8PuOZ7"
      },
      "source": [
        "!du -h \"{path_annotations_target}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeyxRqpx0pU2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}