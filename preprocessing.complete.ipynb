{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "K-means.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3PJO2BJOOem"
      },
      "source": [
        "# Dependencies\n",
        "\n",
        "Use extra UbuntuGIS repository to get GDAL version 3.0.4 or higher, sice Colab's native version 2.2.x is too old for the pipeline.\n",
        "\n",
        "If after installation version of GDAL at the end is still 2.2.x, then restart runtime."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxlMBnn3doy8"
      },
      "source": [
        "# Check container OS version (for correct UbuntuGIS package version)\n",
        "!lsb_release -a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ze9MPcf1Qi8m"
      },
      "source": [
        "## GDAL\n",
        "\n",
        "> If you are working locally and already have GDAL 3.0.4 or above, then just skip or comment the cell below, 'cause it's for Google Colab, which has GDAL 2.2.x only."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v08tDkPxM8X_"
      },
      "source": [
        "# Black magic goes here: installing dependencies for GDAL 3.0.4\n",
        "# build process via APT and install GDAL itself via PyPI\n",
        "!add-apt-repository -y ppa:ubuntugis/ubuntugis-unstable\n",
        "!apt install python3-gdal=3.0.4+dfsg-1~bionic0\n",
        "!apt purge --autoremove python3-gdal\n",
        "!pip install gdal==3.0.4\n",
        "!apt install gdal-bin=3.0.4+dfsg-1~bionic0\n",
        "\n",
        "from osgeo import gdal; print(f\"GDAL version {(gdal.__version__)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iALqZCbkVClj"
      },
      "source": [
        "## MaritimeAI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XS_-JngLkAHW"
      },
      "source": [
        "!pip install git+https://github.com/MaritimeAI/copernicus\n",
        "!pip install git+https://github.com/MaritimeAI/maritimeai\n",
        "!git clone https://github.com/MaritimeAI/resources.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_ZPZcHtaK47"
      },
      "source": [
        "# Common\n",
        "\n",
        "Common part of this pipeline is applicable to (and is used by) all the three parts (snapshot downloading, processing and clustering).\n",
        "\n",
        "\n",
        "## Google Drive\n",
        "\n",
        "Mount _Google Drive_ for SAR images (recomended to store input and output images). If _Google Colab_ is not used, then this cell may be commented out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHBd4tRMJZru"
      },
      "source": [
        "# Google Drive\n",
        "\n",
        "from os import path as osp\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "PATH_DRIVE = osp.join('/', 'content', 'drive')\n",
        "\n",
        "# Do not mount if it is already attached\n",
        "if not osp.exists(PATH_DRIVE):\n",
        "    print(\"Mounting Google Drive...\")\n",
        "    drive.mount(PATH_DRIVE)\n",
        "else:\n",
        "    print(\"Google Drive has been already mounted!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSdXi6F2aWMs"
      },
      "source": [
        "## Paths\n",
        "\n",
        "Paths to be used by preprocessing steps. Use `PATH_STORAGE` as a subdirectory hierarchy to store processed GeoTIFFs/Shapefiles right into _Google Drive_ (empty string will make saving to _Google Drive_'s root into folders `input`/`output`). `PATH_STORAGE` is used with the `PATH_DRIVE` variable only.\n",
        "\n",
        "`PATH_TEMP` is used to store intermediate GeoTIFFs while processing (clustering).\n",
        "\n",
        "`PATH_INPUT` is used as a source of GeoTIFFs (which may already exist in _Google Drive_).\n",
        "\n",
        "`PATH_OUTPUT` is used to save final processed raster and vector images (clustering).\n",
        "\n",
        "`PATH_SNAPSHOTS` is used to store _Sentinel_ .SAFE products (as zip archives).\n",
        "\n",
        "`PATH_RESOURCES` is used as a source of auxiliary files such as GeoJSON search area or Shapefile cutline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4vFyT2LTzN7"
      },
      "source": [
        "from os import path as osp\n",
        "\n",
        "PATH_STORAGE = 'maritime.ai'  # arbitrary subpath in Google Drive (if any)\n",
        "if 'PATH_DRIVE' in locals():\n",
        "    PREFIX_DRIVE = osp.join(osp.basename(PATH_DRIVE), 'MyDrive', PATH_STORAGE)\n",
        "else:\n",
        "    PREFIX_DRIVE = ''\n",
        "\n",
        "PATH_TEMP = osp.join('/', 'content', 'temp')\n",
        "PATH_INPUT = osp.join('/', 'content', PREFIX_DRIVE, 'input')\n",
        "PATH_OUTPUT = osp.join('/', 'content', PREFIX_DRIVE, 'output')\n",
        "PATH_RESOURCES = osp.join('/', 'content', 'resources')\n",
        "PATH_SNAPSHOTS = osp.join('/', 'content', 'snapshots')\n",
        "\n",
        "FILE_SHAPEFILE = osp.join(PATH_RESOURCES, 'clustering', 'cutline',\n",
        "                          'Start_Ice_Map_UTMz40WGS84f_r.shp')\n",
        "\n",
        "print('\\n'.join((PATH_STORAGE, PATH_TEMP, PATH_INPUT, PATH_OUTPUT,\n",
        "                 PATH_RESOURCES, PATH_SNAPSHOTS)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlIo8IC6R7P7"
      },
      "source": [
        "# Part 1. Download satellite images from Copernicus Open Acess Hub\n",
        "\n",
        "A premium account at [Copernicus Open Access Hub](https://scihub.copernicus.eu) gives more access opportunities rather than free account (as usual), so with free account it's possible to get access to the last 45 days of snapshot history.\n",
        "\n",
        "May 4th in example below seems to be inaccessible for free accounts, so it's replaced with the date range of yesterday."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23K0kXh37PKi"
      },
      "source": [
        "%%time\n",
        "\n",
        "import json\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "from copernicus import Config\n",
        "from copernicus import DataHub\n",
        "from copernicus import download\n",
        "from copernicus import Polygons\n",
        "\n",
        "\n",
        "FORMAT_COPERNICUS_DATETIME = '%Y-%m-%dT%H:%M:%S.%f'\n",
        "PATH_AREA_SEARCH = osp.join('resources', 'copernicus',\n",
        "                            'areas', 'pechora.geojson')\n",
        "\n",
        "config = Config()\n",
        "config.username = ''  # <-- set Copernicus Open Access Hub username here\n",
        "config.password = ''  # <-- set Copernicus Open Access Hub password here\n",
        "\n",
        "data_hub = DataHub(config)\n",
        "\n",
        "area = json.load(open(PATH_AREA_SEARCH))\n",
        "search = area['features'][0]['properties'].copy()\n",
        "\n",
        "# Get May 4th of the current year (full day) - example\n",
        "now = datetime.now().replace(month=5, day=4, hour=0, minute=0,\n",
        "                             second=0, microsecond=0)\n",
        "day = timedelta(hours=23, minutes=59,\n",
        "                seconds=59, microseconds=999999)\n",
        "\n",
        "date_start = now.strftime(FORMAT_COPERNICUS_DATETIME)[:-3] + 'Z'\n",
        "date_stop = (now + day).strftime(FORMAT_COPERNICUS_DATETIME)[:-3] + 'Z'\n",
        "\n",
        "# Update time range for yesterday\n",
        "del search['filenames']\n",
        "search.update({\n",
        "    'start': 0,\n",
        "    'platformName': 'Sentinel-1',\n",
        "    'productType': 'GRD',\n",
        "    # 'beginPosition': f\"[{date_start} TO {date_stop}]\",\n",
        "    'beginPosition': f\"[NOW-2DAYS TO NOW-1DAYS]\",\n",
        "})\n",
        "print(f\"DEBUG: search = {search}\")\n",
        "\n",
        "polygon, properties = Polygons.read_geojson(PATH_AREA_SEARCH)\n",
        "snapshots = data_hub.search(search, area=polygon)\n",
        "# print(f\"DEBUG: snapshots = {snapshots}\")\n",
        "\n",
        "config.output = PATH_SNAPSHOTS\n",
        "\n",
        "for i, snapshot in enumerate(snapshots):\n",
        "    print(f\"{i:03d}\", snapshot.link)\n",
        "    download(snapshot.link, config)\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmWpTkSJSKij"
      },
      "source": [
        "# Part 2. Process archives from Copernicus Open Acess Hub\n",
        "\n",
        "Setting arguments `ratio=True` or/and `negative=True` in `process_sentinel1` function may consume a lot of RAM, so it may not be enough in Google Colab with 12 Gb RAM.\n",
        "\n",
        "> Some snapshots may appear black after processing. That's because a small data area in a corner was cut off with Shapefile cutline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZa6hf3CUavd"
      },
      "source": [
        "%%time\n",
        "\n",
        "from glob import glob\n",
        "from zipfile import BadZipFile\n",
        "\n",
        "from maritimeai import process_sentinel1\n",
        "\n",
        "if osp.isdir(PATH_SNAPSHOTS):\n",
        "    for filename in glob(osp.join(PATH_SNAPSHOTS, '*.zip')):\n",
        "        try:\n",
        "            process_sentinel1(filename, PATH_INPUT, area='default',\n",
        "                            shapes=[FILE_SHAPEFILE], negative=False)\n",
        "        except BadZipFile:\n",
        "            print(f\"ERROR: {filename} is damaged!\")\n",
        "else:\n",
        "    raise FileNotFoundError(f\"Path '{PATH_SNAPSHOTS}' must exist!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHrsXUB3_Jzg"
      },
      "source": [
        "# Part 3. Unsupervised annotation\n",
        "\n",
        "This part takes a snapshot and does k-means clustering on it. Clustered snapshot as an output is a vector shapefile, which can be further processed manually in _ArcGIS_.\n",
        "\n",
        "> Use `gdalinfo` or `ogrinfo` to see metadata from raster and vector geospatial images respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aetIQTxr3NQi"
      },
      "source": [
        "%%time\n",
        "\n",
        "import os\n",
        "\n",
        "from datetime import datetime\n",
        "from glob import glob\n",
        "from os import path as osp\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from osgeo import ogr, gdal, gdalconst\n",
        "\n",
        "from maritimeai import read_to_channels\n",
        "from maritimeai import enhance_dataset\n",
        "from maritimeai import save_grayscale\n",
        "from maritimeai import cluster_dataset\n",
        "from maritimeai import vectorize_dataset\n",
        "\n",
        "from maritimeai.utils import GDALCallback\n",
        "\n",
        "from maritimeai import CHANNELS_RGB\n",
        "\n",
        "\n",
        "FORMAT_DATE = '%Y-%m-%d-%H-%M-%S'\n",
        "NUM_CLUSTERS = 7 # K-means\n",
        "SHOW_HIST = False\n",
        "\n",
        "if osp.isdir(PATH_INPUT):\n",
        "    os.makedirs(PATH_OUTPUT, exist_ok=True)\n",
        "else:\n",
        "    raise FileNotFoundError(f\"Path '{PATH_INPUT}' must exist!\")\n",
        "\n",
        "if not osp.isfile(FILE_SHAPEFILE):\n",
        "    raise FileNotFoundError(f\"Shapefile '{FILE_SHAPEFILE}' must exist!\")\n",
        "\n",
        "filenames = glob(osp.join(PATH_INPUT, '*', '*', '*.tiff'))\n",
        "print(f\"Source files -->\\n\")\n",
        "print('\\n'.join(filenames))\n",
        "print(f\"Source shape is {FILE_SHAPEFILE}\")\n",
        "# !gdalinfo \"{filenames[0]}\"\n",
        "\n",
        "try:\n",
        "    # To shake your shape like a sine wave\n",
        "    if osp.isfile(FILE_SHAPEFILE):\n",
        "        shape = osp.abspath(osp.realpath(FILE_SHAPEFILE))\n",
        "    else:\n",
        "        raise FileNotFoundError\n",
        "except (TypeError, FileNotFoundError) as e:\n",
        "    print(f\"Shapefile '{FILE_SHAPEFILE}' does not exist!\")\n",
        "    shape = None\n",
        "print(f\"Available shape is {shape}\")\n",
        "# !ogrinfo \"{shape}\"\n",
        "\n",
        "assert int(gdal.__version__.split('.')[0]) >= 3, f\"Required GDAL version >=3.0!\"\n",
        "gdal.UseExceptions()\n",
        "\n",
        "timestamp = datetime.utcnow().strftime(FORMAT_DATE)\n",
        "\n",
        "print(f\"\\nProcessing files -->\")\n",
        "for filename in filenames:\n",
        "    # Input images assumed to be grayscale\n",
        "    print(f\"\\nInput file is {filename}\")\n",
        "    output, _ = osp.splitext(filename.replace(PATH_INPUT,\n",
        "                                              osp.join(PATH_OUTPUT, timestamp)))\n",
        "    print(f\"Output is {output}\")\n",
        "    os.makedirs(osp.dirname(output), exist_ok=True)\n",
        "    temp = filename.replace(PATH_INPUT, PATH_TEMP)\n",
        "    print(f\"Temporary filename is {temp}\")\n",
        "    os.makedirs(osp.dirname(temp), exist_ok=True)\n",
        "\n",
        "    # Create temporary RGB GeoTIFF\n",
        "    if 'dataset' in locals():\n",
        "        del dataset\n",
        "    dataset = read_to_channels(filename, CHANNELS_RGB)\n",
        "    print(f\"Warping {filename} into {temp}...\")\n",
        "    gdal.Warp(temp, dataset, format='GTiff', dstSRS='EPSG:32640',\n",
        "              srcNodata=0, dstNodata=0, xRes=40, yRes=40,\n",
        "              cutlineDSName=f\"{shape}\", cropToCutline=(True if shape else False),\n",
        "              creationOptions=['COMPRESS=DEFLATE'], callback=GDALCallback())\n",
        "\n",
        "    # Process temporary RGB GeoTIFF\n",
        "    dataset = enhance_dataset(temp, bilateral=(7, 15, 15))\n",
        "    try:\n",
        "\n",
        "        # Save temporary dataset to destination (tile-wise filtering result)\n",
        "        destination = osp.join(output, 'image')\n",
        "        os.makedirs(destination, exist_ok=True)\n",
        "        destination = osp.join(destination, osp.basename(temp))\n",
        "        name, extension = osp.splitext(destination)\n",
        "        destination = f\"{name}_warped{extension}\"\n",
        "\n",
        "        save_grayscale(dataset, destination, callback=GDALCallback())\n",
        "\n",
        "        # Try to process the whole RGB image at once\n",
        "        # Clustering tile-wise is a bad idea\n",
        "        cluster_dataset(dataset, clusters=NUM_CLUSTERS,\n",
        "                        plt=plt, show_histogram=SHOW_HIST)\n",
        "\n",
        "        # Save RGB dataset to destination (output raster file)\n",
        "        destination = osp.join(output, 'image')\n",
        "        os.makedirs(destination, exist_ok=True)\n",
        "        destination = osp.join(destination, osp.basename(temp))\n",
        "        name, extension = osp.splitext(destination)\n",
        "        destination = f\"{name}_clustered{extension}\"\n",
        "\n",
        "        print(f\"Saving image to {destination}...\")\n",
        "        gdal.Translate(destination, dataset, options=['-co', 'COMPRESS=DEFLATE'],\n",
        "                       callback=GDALCallback())\n",
        "\n",
        "        # Vectorize clusters (create output shapefile(s) from clustering)\n",
        "        # WARNING: shapefile may be quite large\n",
        "        destination = osp.join(output, 'shape')\n",
        "        os.makedirs(destination, exist_ok=True)\n",
        "        destination = osp.join(destination, osp.basename(temp))\n",
        "        destination = osp.splitext(destination)[0] + '.shp'\n",
        "\n",
        "        vectorize_dataset(dataset, destination, GDALCallback())\n",
        "    finally:\n",
        "        try:\n",
        "            dataset.FlushCache()\n",
        "            del dataset\n",
        "        except (NameError, AttributeError):\n",
        "            pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuxCit4JI86W"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}