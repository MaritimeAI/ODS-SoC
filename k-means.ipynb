{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "K-means.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3PJO2BJOOem",
        "colab_type": "text"
      },
      "source": [
        "# Dependencies\n",
        "\n",
        "Use extra UbuntuGIS repository to get GDAL version 3.0 or higher, sice Colab's native version 2.2.3 is too old for the pipeline.\n",
        "\n",
        "If after installation version of GDAL at the end is still 2.2.3, then restart runtime."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v08tDkPxM8X_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!add-apt-repository -y ppa:ubuntugis/ubuntugis-unstable\n",
        "!apt install python3-gdal=3.0.4+dfsg-1~bionic0\n",
        "\n",
        "from osgeo import gdal; print(f\"GDAL version {(gdal.__version__)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iegbHzklOKVs",
        "colab_type": "text"
      },
      "source": [
        "# Google Drive\n",
        "\n",
        "Mount Google Drive to access SAR images (recomended to store input and output images). If you're not working with Google Colab, then you probably do not need this cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHBd4tRMJZru",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Google Drive\n",
        "\n",
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "# Do not mount if it is already attached\n",
        "if not os.path.exists(\"/content/drive\"):\n",
        "    drive.mount(\"/content/drive\")\n",
        "    # print(f\"Mounted Google Drive\")\n",
        "else:\n",
        "    print(f\"Google Drive is already mounted\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lH6l7d-8OG3m",
        "colab_type": "text"
      },
      "source": [
        "# Functions\n",
        "\n",
        "Functions cell. Just like functions stored in _utils.py_ file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwnuInlzH-g-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "\n",
        "from typing import List, Tuple, Union\n",
        "\n",
        "\n",
        "def gdal_callback(completed: float, message: str, args: None) -> bool:\n",
        "    progress = int(completed * 100)\n",
        "    if progress == 100:\n",
        "        print(progress, '- done.')\n",
        "    elif progress % 10 == 0:\n",
        "        print(progress, end='', flush=True)\n",
        "    elif progress % 2 == 0:\n",
        "        print('.', end='', flush=True)\n",
        "    return True\n",
        "\n",
        "\n",
        "def adjust_gamma(image: np.ndarray, gamma: float = 1.1,\n",
        "                 pad: Union[int, Tuple[int, int]] = 1) -> np.ndarray:\n",
        "    assert image.ndim == 2, f\"Input image must be grayscale!\"\n",
        "    if type(pad) is Tuple[int, int]:\n",
        "        pad_low, pad_high = pad\n",
        "    else:\n",
        "        pad_low, pad_high = (pad, 0)\n",
        "    lut = ((np.linspace(pad_low, 255 - pad_high, 256) / 255) ** (1 / gamma) *\n",
        "           255).round().astype(np.uint8)\n",
        "    return cv.LUT(image, lut).astype(np.uint8)\n",
        "\n",
        "\n",
        "def apply_kmeans(image: np.ndarray, num_clusters: int, cycles: int = 10,\n",
        "                 iters: int = 10, eps: float = 0.9,\n",
        "                 mask: int = 255) -> np.ndarray:\n",
        "    assert image.ndim == 2, f\"Image must be 2D, but {image.ndim}D is given!\"\n",
        "    # Samples are the float32 image with the mask color dropped to zero\n",
        "    samples = (np.float32(image) *\n",
        "                (image != mask).astype(np.uint8)).reshape(-1, 1)\n",
        "    criteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, iters, eps)\n",
        "    _, labels, centers = cv.kmeans(samples, NUM_CLUSTERS, None, criteria,\n",
        "                                    10, cv.KMEANS_PP_CENTERS)\n",
        "    spread = np.linspace(0, 255, centers.shape[0] + 1)\\\n",
        "                        [centers.argsort(axis=0)].round().astype(np.uint8)\n",
        "    return spread[labels.flatten()].reshape(image.shape)\n",
        "\n",
        "\n",
        "def extend_mask(image: np.ndarray, n: int = -5, color: int = 255) -> np.ndarray:\n",
        "    lut = np.arange(256, dtype=np.uint8)\n",
        "    if n > 0:\n",
        "        lut[:n] = color\n",
        "    elif n < 0:\n",
        "        lut[n:] = color\n",
        "    return cv.LUT(image, lut).astype(np.uint8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loi_R-MIN2-3",
        "colab_type": "text"
      },
      "source": [
        "# Processing\n",
        "\n",
        "Main processing loop. Change the following variables for yourself:\n",
        "* `PATH_INPUT` - where input SAR images are stored, that may be any level of depth, sice they are listed with `glob`;\n",
        "* `PATH_TEMP` - where to store temporary dataset, and it's recommended to use locations under _/content_, when using Google Colab;\n",
        "* `PATH_OUTPUT` - to write output files to (subdirectories are created automatically), and it's recommended to use Google Drive (existing files in output directory will be overwritten);\n",
        "* `GDAL_MAX_RAM` (optional) - limit cache size for reading GDAL datasets (default is 10 MiB);\n",
        "* `GDAL_TILE_SIZE` (optional) - limit tile size when processing tile-by-tile, e.g. for bilateral filtering of gamma correction (clustering is supposed to be done over the whole imgage).\n",
        "\n",
        "Also note to change `glob` pattern for `files` variable to match your directory structure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLOAe9RT2Tsk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from glob import glob\n",
        "from matplotlib import cm\n",
        "from osgeo import ogr, gdal, gdalconst\n",
        "\n",
        "\n",
        "PATH_TEMP = '/content/temp'\n",
        "PATH_INPUT = '/content/drive/My Drive/Colab Notebooks/SAR Processing/input'\n",
        "PATH_OUTPUT = '/content/drive/My Drive/Colab Notebooks/SAR Processing/output'\n",
        "FILE_SHAPEFILE = PATH_INPUT + '/Start_Ice_Map_UTMz40WGS84f_r.shp'\n",
        "\n",
        "GDAL_MAX_RAM = 600 * 1024 * 1024 # 600 MiB\n",
        "GDAL_TILE_SIZE = 2048 # 3200 is maximum (Google Colab)\n",
        "NUM_CLUSTERS = 7 # K-means\n",
        "NUM_CHANNELS = 3 # RGB output\n",
        "GAMMA = 1.0 # 1.0 - unchanged\n",
        "\n",
        "if os.path.isdir(PATH_INPUT):\n",
        "    os.makedirs(PATH_OUTPUT, exist_ok=True)\n",
        "else:\n",
        "    raise FileNotFoundError(f\"Path '{PATH_INPUT}' must exist!\")\n",
        "\n",
        "if not os.path.isfile(FILE_SHAPEFILE):\n",
        "    raise FileNotFoundError(f\"Shapefile '{FILE_SHAPEFILE}' must exist!\")\n",
        "\n",
        "files = glob(os.path.join(PATH_INPUT, 'HH', '*.tif'))\n",
        "print(f\"Source files -->\\n\")\n",
        "print('\\n'.join(files))\n",
        "print(f\"Source shape is {FILE_SHAPEFILE}\")\n",
        "# !gdalinfo \"{files[0]}\"\n",
        "\n",
        "try:\n",
        "    # To shake your shape like a sine wave\n",
        "    if os.path.isfile(FILE_SHAPEFILE):\n",
        "        shape = os.path.abspath(os.path.realpath(FILE_SHAPEFILE))\n",
        "    else:\n",
        "        raise FileNotFoundError\n",
        "except (TypeError, FileNotFoundError) as e:\n",
        "    print(f\"Shapefile '{FILE_SHAPEFILE}' does not exist!\")\n",
        "    shape = None\n",
        "print(f\"Available shape is {shape}\")\n",
        "# !ogrinfo \"{shape}\"\n",
        "\n",
        "assert int(gdal.__version__.split('.')[0]) >= 3, f\"Required GDAL version >=3.0!\"\n",
        "gdal.UseExceptions()\n",
        "\n",
        "# Get colormap from Matplotlib to colorize clusters\n",
        "colormap = (cm.terrain(range(256)) * 255).round().astype(np.uint8)\n",
        "\n",
        "print(f\"\\nProcessing files -->\")\n",
        "for filename in files:\n",
        "    # Input images assumed to be grayscale\n",
        "    print(f\"\\nInput file is {filename}\")\n",
        "    output, _ = os.path.splitext(filename.replace(PATH_INPUT, PATH_OUTPUT))\n",
        "    print(f\"Output is {output}\")\n",
        "    os.makedirs(os.path.dirname(output), exist_ok=True)\n",
        "    temp = filename.replace(PATH_INPUT, PATH_TEMP)\n",
        "    print(f\"Temporary filename is {temp}\")\n",
        "    os.makedirs(os.path.dirname(temp), exist_ok=True)\n",
        "\n",
        "    # Create temporary RGB GeoTIFF\n",
        "    if 'dataset' in locals():\n",
        "        del dataset\n",
        "    dataset = gdal.Open(filename, gdal.GA_ReadOnly)\n",
        "    if dataset.RasterCount < 1:\n",
        "        # raise AttributeError(f\"Source dataset has no rasters!\")\n",
        "        print(f\"ERROR: dataset {filename} has no rasters! Skipping...\")\n",
        "        continue\n",
        "    if dataset.RasterCount < NUM_CHANNELS:\n",
        "        channels = [gdal.GCI_RedBand, gdal.GCI_GreenBand,\n",
        "                    gdal.GCI_BlueBand, gdal.GCI_AlphaBand]\n",
        "        tempset = gdal.GetDriverByName('MEM').CreateCopy('', dataset, 0)\n",
        "        band = tempset.GetRasterBand(1)\n",
        "        layer: np.ndarray = band.ReadAsArray()\n",
        "        band.SetColorInterpretation(channels[0])\n",
        "        for i in range(tempset.RasterCount + 1, NUM_CHANNELS + 1):\n",
        "            tempset.AddBand()\n",
        "            band = tempset.GetRasterBand(i)\n",
        "            band.WriteArray(layer)\n",
        "            band.SetColorInterpretation(channels[i - 1])\n",
        "        del band\n",
        "        # Change projection and resolution\n",
        "        options = gdal.WarpOptions(format='GTiff', dstSRS='EPSG:32640',\n",
        "                                srcNodata=255, dstNodata=255,# geoloc=False,\n",
        "                                xRes=40, yRes=40, cutlineDSName=f\"{shape}\",\n",
        "                                cropToCutline=(True if shape else False),\n",
        "                                callback=gdal_callback)\n",
        "        print(f\"Warping source file into {temp}...\")\n",
        "        gdal.Warp(temp, tempset, options=options)\n",
        "        del tempset\n",
        "    del dataset\n",
        "\n",
        "    # Process temporary RGB GeoTIFF\n",
        "    dataset = gdal.Open(temp, gdal.GA_Update)\n",
        "    try:\n",
        "        if dataset.RasterCount < 1:\n",
        "            raise AttributeError(f\"Temp dataset has no rasters!\")\n",
        "        else:\n",
        "            print(f\"GeoTIFF rasters = {dataset.RasterCount}\")\n",
        "        print(f\"Dataset raster size = ({dataset.RasterYSize}, {dataset.RasterXSize})\")\n",
        "\n",
        "        # Try to process RGB image tile-by-tile\n",
        "        if not type(GDAL_TILE_SIZE) is int or GDAL_TILE_SIZE > 3072:\n",
        "            GDAL_TILE_SIZE = 3072\n",
        "        tile_x_size = GDAL_TILE_SIZE\n",
        "        tile_y_size = GDAL_TILE_SIZE\n",
        "        tiles = dataset.GetTiledVirtualMemArray(eAccess=gdalconst.GF_Write,\n",
        "                                            tilexsize=tile_x_size,\n",
        "                                            tileysize=tile_y_size,\n",
        "                                            cache_size=GDAL_MAX_RAM,\n",
        "                                            tile_organization=gdalconst.GTO_TIP)\n",
        "        try:\n",
        "            print(f\"Tiles array shape = {tiles.shape}\",\n",
        "                  \"(tilesY, tilesX, Y, X, channels)\")\n",
        "            # Kernels: rect, cross, ellipse (morphology)\n",
        "            # kernel = cv.getStructuringElement(cv.MORPH_ELLIPSE, (9, 9))\n",
        "            print(f\"Preprocessing...\")\n",
        "            for i in range(tiles.shape[0]):\n",
        "                for j in range(tiles.shape[1]):\n",
        "                    correction = tiles[i, j, ..., 0]\n",
        "                    correction = extend_mask(correction, -32)\n",
        "                    if GAMMA != 1.0:\n",
        "                        correction = adjust_gamma(correction, gamma=GAMMA, pad=0)\n",
        "                    correction = cv.bilateralFilter(correction, 31, 63, 63)\n",
        "                    # _, correction = cv.threshold(correction, 1, 255, cv.THRESH_BINARY)\n",
        "                    # correction = cv.morphologyEx(correction, cv.MORPH_CLOSE, kernel)\n",
        "                    tiles[i, j, ...] = np.repeat(correction[..., None], 3, axis=-1)\n",
        "                    print('.', end='', flush=True)\n",
        "                print()\n",
        "        finally:\n",
        "            del tiles\n",
        "\n",
        "        # Try to process the whole RGB image at once\n",
        "        image = dataset.GetVirtualMemArray(eAccess=gdalconst.GF_Write,\n",
        "                                           cache_size=GDAL_MAX_RAM,\n",
        "                                           band_sequential=False)\n",
        "        try:\n",
        "            print(f\"Clustering {image.shape} image...\")\n",
        "            correction = image[..., 0]\n",
        "            plt.subplots(1, 1, figsize=(25, 10))[1].imshow(correction, cmap='gray')\n",
        "            histograms = plt.subplots(1, 1, figsize=(25, 10))[1]\n",
        "            histograms.hist(correction.ravel(), [127], [1, 255])\n",
        "            histograms.set_xlim([0, 127])\n",
        "            clustered = apply_kmeans(correction, NUM_CLUSTERS, eps=0.95)\n",
        "            print(f\"Done!\")\n",
        "\n",
        "            # Assemble into channels with colormap applied\n",
        "            clustered = np.stack([cv.LUT(clustered, colormap[..., 0]) |\n",
        "                                  (correction == 255).astype(np.uint8) * 255,\n",
        "                                  cv.LUT(clustered, colormap[..., 1]) |\n",
        "                                  (correction == 255).astype(np.uint8) * 255,\n",
        "                                  cv.LUT(clustered, colormap[..., 2]) |\n",
        "                                  (correction == 255).astype(np.uint8) * 255],\n",
        "                                 axis=2)\n",
        "            image[...] = clustered\n",
        "            plt.subplots(1, 1, figsize=(25, 10))[1].imshow(clustered)\n",
        "        finally:\n",
        "            del image\n",
        "\n",
        "        # Save temporary dataset to destination (output raster file)\n",
        "        destination = os.path.join(output, 'image')\n",
        "        os.makedirs(destination, exist_ok=True)\n",
        "        destination = os.path.join(destination, os.path.basename(temp))\n",
        "        print(f\"Saving image to {destination}...\")\n",
        "        gdal.Translate(destination, dataset)\n",
        "\n",
        "        # Vectorize clusters (create output shapefile set for clusters)\n",
        "        # WARNING: shapefile may be quite large\n",
        "        destination = os.path.join(output, 'shape')\n",
        "        os.makedirs(destination, exist_ok=True)\n",
        "        destination = os.path.join(destination, os.path.basename(temp))\n",
        "        destination = os.path.splitext(destination)[0] + '.shp'\n",
        "\n",
        "        tempset = ogr.GetDriverByName('ESRI Shapefile').CreateDataSource(destination)\n",
        "        try:\n",
        "            band_source = dataset.GetRasterBand(1)\n",
        "            band_mask = band_source.GetMaskBand()\n",
        "            srs = dataset.GetSpatialRef()\n",
        "\n",
        "            layer = tempset.CreateLayer('out', geom_type=ogr.wkbPolygon, srs=srs)\n",
        "            layer.CreateField(ogr.FieldDefn('DN', ogr.OFTInteger))\n",
        "\n",
        "            options = []\n",
        "            field = 0\n",
        "\n",
        "            print(f\"Saving shape to {destination}...\")\n",
        "            gdal.Polygonize(band_source, band_mask, layer, field, options,\n",
        "                            callback=gdal_callback)\n",
        "        finally:\n",
        "            del tempset\n",
        "\n",
        "        # Calculate final histogram (optional) - shall be less than\n",
        "        # NUM_CLUSTERS peaks for each channel (R, G, B)\n",
        "        print(f\"Building histograms...\")\n",
        "        histograms = plt.subplots(1, 1, figsize=(25, 10))[1]\n",
        "        for i, c in enumerate(['b', 'g', 'r']):\n",
        "            histogram = cv.calcHist([clustered], [i], None, [127], [1, 255])\n",
        "            histograms.plot(histogram, color=c)\n",
        "            histograms.set_xlim([0, 127])\n",
        "        plt.show()\n",
        "    finally:\n",
        "        del dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lUlxECT2YDf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !7z a -mx=9 -ms=on \"/content/{os.path.basename(output)}.7z\" \"{output}\"\n",
        "# !cp /content/*.7z \"{output}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRrPjuz-By87",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !ogrinfo \"{os.path.splitext(destination)[0] + '.shp'}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyZK220VyVxC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !uname -a\n",
        "# !getconf PAGE_SIZE\n",
        "# !df -h /dev/shm\n",
        "# !free -h"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}